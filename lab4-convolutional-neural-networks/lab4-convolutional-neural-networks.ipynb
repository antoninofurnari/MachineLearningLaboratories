{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".rendered_html * + p, .rendered_html p {\n",
    "    text-align:justify;\n",
    "}\n",
    ".print {\n",
    "    display:none;\n",
    "}\n",
    ".highlight {\n",
    "    background:white;\n",
    "}\n",
    "@media print {\n",
    " a[href]:after {\n",
    "     content: \"\"\n",
    " }\n",
    " .noprint {\n",
    "  display:none\n",
    "  }\n",
    "  .print {\n",
    "        display:block;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "<head>\n",
    "    <base target=\"_blank\">\n",
    "</head>\n",
    "<div style=\"text-align:left\"><a href=\"http://web.dmi.unict.it/\"><img src=\"img/dmi.png\" style=\"width:300px; margin:0;\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://iplab.dmi.unict.it/\"><img src=\"img/iplab.png\" style=\"width:900px\"></a>\n",
    "<center><h2>Machine Learning - A.A. 2020-2021</h2></center>\n",
    "<center><h3>Convolutional Neural Networks</h3></center>\n",
    "<br>\n",
    "<center>Antonino Furnari - <a href=\"http://www.antoninofurnari.it/\" target=\"_blank\">http://www.antoninofurnari.it/</a> - <a href=\"mailto:furnari@dmi.unict.it\">furnari@dmi.unict.it</a> </center>\n",
    "<center>Giovanni Maria Farinella - <a href=\"http://www.dmi.unict.it/farinella/\" target=\"_blank\">http://www.dmi.unict.it/farinella/</a> - <a href=\"mailto:gfarinella@dmi.unict.it\">gfarinella@dmi.unict.it</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Convolutional Neural Networks (CNN) permettono di applicare le reti neural in maniera efficiente al processamento di immagini. Abbiamo già visto come sia possibile classificare immagini mediante un regressore softmax o un multilayer perceptron. Tuttavia, vista l'alta dimensionalità delle immagini, tali metodi tendono a non scalare a immagini di grandi dimensioni e a grossi dataset di immagini. Le Convolutional Neural Networks cercano di risolvere questi problemi sostituendo le transformazioni lineari con le convoluzioni, che richiedono meno parametri e presentano invarianza traslazionale. \n",
    "\n",
    "**NOTA**: In questo laboratorio, i modelli vengono allenati per 50/150 epoche. Questi parametri richiedono lunghi tempi di allenamento. Ai fini del completamento del laboratorio in aula, si consiglia di allenare i modelli solo per qualche epoca (a fini didattici) e di ripetere tutti gli addestramenti una volta a casa (il processo richiederà delle ore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 LeNet\n",
    "Iniziamo implementando un modello molto simile a LeNet-5, il primo esempio di CNN presente in letteratura (LeCunn et al. 1998). Il modello è stato proposto nel 1998 per risolvere il problema della classificazione di cifre del dataset MNIST-DIGITS. Il modello qui presentato differisce da quello originale in diversi dettagli, ma la sua struttura resta fedele all'originale. Il modello prevede sette layers:\n",
    "\n",
    " * Un layer di convoluzione C1. Il layer prende in input una immagine di dimensione $28 \\times 28$ pixels e un unico canale. Mediante l'applicazione di $6$ kernel di convoluzione di dimensioni $5 \\times 5$, vengono calcolate $6$ mappe di features. Nessun tipo di padding viene applicato, dunque ogni mappa di features ha dimensioni $24 \\times 24$. In pratica, l'input del layer C1 è un tensore di dimensione $1 \\times 28 \\times 28$, mentre il suo output è un tensore di dimensione $6 \\times 24 \\times 24$. Ogni convoluzione viene effettuata applicando un kernel di dimensione $5 \\times 5$ e sommando al risultato un termine di bias. Il numero totale di parametri ottimizzabili del livello C1 è dunque pari a $6 \\cdot (5 \\cdot 5 +1) = 156$;\n",
    " * Un layer di sottocampionamento S2. Il layer prende in input le 6 mappe $24 \\times 24$ e le sottocampiona ottenendo in output mappe di dimensioni $12 \\times 12$. Il sottocampionamento è effettuato suddividendo la mappa in input in intorni $2 \\times 2$ e calcolando il valore medio di ognuno di questi intorni (average pooling); In pratica, l'input del layer è un tensore di dimensione $6 \\times 24 \\times 24$, mentre il suo output è un tensore di dimensione $6 \\times 12 \\times 12$. Il layer non contiene alcun parametro ottimizzabile;\n",
    " * Un layer di convoluzione C3. Il layer prende in input il tensore di dimensioni $6 \\times 12 \\times 12$ e calcola $16$ mappe di feature di dimensioni $8 \\times 8$ utilizzando kernel di dimensioni $5 \\times 5$. Il numero di parametri ottimizzabili è pari a $6 \\times 5 \\times 5 \\times 16 + 16= 2416$;\n",
    " * Un layer di sottocampionamento S4. In maniera del tutto simile al layer S2, prende in input $16$ mappe di dimensione $8 \\times 8$ e produce $16$ mappe di dimensione $4 \\times 4$. Anche questo livello non contiene parametri ottimizzabili;\n",
    " * Un layer di trasformazione lineare \"fully connected\" F5. Il layer prende in input il tensore di dimensioni $16 \\times 4 \\times 4$. Il tensore viene dunque considerato come un vettore monodimensionale di $256$ unità e trasformato (mediante trasformazione lineare) in un tensore di $120$ unità. Il numero totale di parametri è dato da $120 \\cdot 256 + 120 = 30840$ (l'ultimo \"+120\" rappresenta i parametri di bias);\n",
    " * Un layer di trasformazione lineare F6. Il layer prende in input il vettore di $120$ unità e lo trasforma in un vettore di $84$ unità. Il numero di parametri ottimizzabili è pari a $84 \\times 120 + 84 = 10164$;\n",
    " * Un layer di trasformazione lineare F7. Il layer prende in input il vettore di $84$ unità e lo trasforma in un vettore di $10$ unità (gli score relativi alle $10$ classi di MNIST-DIGITS). Il numero di parametri ottimizzabili del livello è pari a $84*10+10 = 850$.\n",
    " \n",
    "Il numero totale di parametri del modello è pari a $44426$. Tra ogni coppia di layer, tranne che subito dopo i layer di sottocampionamento, è presente una attivazione di tipo TanH. la figura mostra uno schema del modello LeNet considerato:\n",
    "\n",
    "<center><img src =\"img/lenet_mod.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 1**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Perchè i layer di convoluzione (C1, C3) riducono le dimensioni delle mappe in input? Secondo quale formula le dimensioni vengono ridotte?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 1**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 2**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti il numero di parametri di LeNet con quelli di un multilayer perceptron con $64$ unità nascoste. Quale dei due modelli contiene meno parametri? A cosa è dovuta la differenza?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 2**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di iniziare a scrivere del codice, impostiamo un seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(1328)\n",
    "torch.random.manual_seed(1328);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementiamo adesso il modello LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__() \n",
    "        #Definiamo il primo livello. Dobbiamo effettuare una convoluzione 2D (ovvero su immagini)\n",
    "        #Utilizziamo il modulo Conv2d che prende in input:\n",
    "        #  - il numero di canali in input: 1 (si tratta di immagini in scala di grigio)\n",
    "        #  - il numero di canali in output: 6 (le mappe di feature)\n",
    "        #  - la dimensione del kernel: 5 (sta per \"5 X 5\")\n",
    "        self.C1 = nn.Conv2d(1, 6, 5)\n",
    "        #Definiamo il livello di subsampling. Questo viene implementato usando il modulo \"AvgPool2d\"\n",
    "        #Il modulo richiede in input la dimensione dei neighborhood rispetto ai quali calcolare\n",
    "        # i valori medi: 2\n",
    "        self.S2 = nn.AvgPool2d(2)\n",
    "        #Definiamo il livello C3 in maniera analoga a quanto fatto per il livello C1:\n",
    "        self.C3 = nn.Conv2d(6, 16, 5)\n",
    "        #Definiamo il successivo max pooling 2d\n",
    "        self.S4 = nn.AvgPool2d(2)\n",
    "        #Definiamo il primo layer FC\n",
    "        self.F5 = nn.Linear(256, 120)\n",
    "        #Definiamo il secondo layer FC\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "        #Definiamo il terzo layer FC\n",
    "        self.F7 = nn.Linear(84, 10)\n",
    "        \n",
    "        #Definiamo inoltre un modulo per calcolare l'attivazione Tanh\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.C1(x)\n",
    "        x = self.S2(x)\n",
    "        x = self.activation(x) #inseriamo le attivazioni ove opportuno\n",
    "        x = self.C3(x)\n",
    "        x = self.S4(x)\n",
    "        x = self.activation(x) #inseriamo le attivazioni ove opportuno\n",
    "        x = self.F5(x.view(x.shape[0],-1)) #dobbiamo effettuare un \"reshape\" del tensore\n",
    "        x = self.activation(x)\n",
    "        x = self.F6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.F7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo il modello e verifichiamone il numero di parametri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44426"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LeNet()\n",
    "sum([p.numel() for p in net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il dataset MNIST-DIGITS e definiamo i loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist_train = MNIST(root='mnist',train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root='mnist',train=False, download=True, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=1024, num_workers=2, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo l'oggetto `AverageValueMeter` come visto nello scorso laboratorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageValueMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "    \n",
    "    def add(self, value, num):\n",
    "        self.sum += value*num\n",
    "        self.num += num\n",
    "        \n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la procedura di training vista nel laboratorio precedente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from os.path import join\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.01, epochs=10, momentum=0.99, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum) \n",
    "    #meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    #writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    #device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    #definiamo un dizionario contenente i loader di training e test\n",
    "    loader = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    #inizializziamo il global step\n",
    "    global_step = 0\n",
    "    for e in range(epochs):\n",
    "        #iteriamo tra due modalità: train e test\n",
    "        for mode in ['train','test']:\n",
    "            loss_meter.reset(); acc_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'): #abilitiamo i gradienti solo in training\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    x=batch[0].to(device) #\"portiamoli sul device corretto\"\n",
    "                    y=batch[1].to(device)\n",
    "                    output = model(x)\n",
    "                    \n",
    "                    #aggiorniamo il global_step\n",
    "                    #conterrà il numero di campioni visti durante il training\n",
    "                    n = x.shape[0] #numero di elementi nel batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output,y)\n",
    "\n",
    "                    if mode=='train':\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    acc = accuracy_score(y.to('cpu'),output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(),n)\n",
    "                    acc_meter.add(acc,n)\n",
    "\n",
    "                    #loggiamo i risultati iterazione per iterazione solo durante il training\n",
    "                    if mode=='train':\n",
    "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "            #una volta finita l'epoca (sia nel caso di training che test, loggiamo le stime finali)\n",
    "            writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "            \n",
    "        #conserviamo i pesi del modello alla fine di un ciclo di training e test\n",
    "        torch.save(model.state_dict(),'%s-%d.pth'%(exp_name,e+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo e alleniamo la rete LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lenet_mnist = LeNet()\n",
    "lenet_mnist = train_classifier(lenet_mnist, mnist_train_loader, mnist_test_loader, 'lenet_mnist', epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/lenet_mnist_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 3**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si osservi il log della procedura di training. Possiamo dire che la rete converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 3**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo la consueta funzione per ottenere probabilità di test predette dal modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        output = model(x)\n",
    "        preds = output.to('cpu').max(1)[1].numpy()\n",
    "        labs = y.to('cpu').numpy()\n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo le accuracy di training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy di training: 1.0000\n",
      "Accuarcy di test: 0.9789\n"
     ]
    }
   ],
   "source": [
    "lenet_mnist_predictions_train, mnist_labels_train = test_classifier(lenet_mnist, mnist_train_loader)\n",
    "lenet_mnist_predictions_test, mnist_labels_test = test_classifier(lenet_mnist, mnist_test_loader)\n",
    "print(\"Accuarcy di training: %0.4f\"% accuracy_score(mnist_labels_train, lenet_mnist_predictions_train))\n",
    "print(\"Accuarcy di test: %0.4f\"% accuracy_score(mnist_labels_test, lenet_mnist_predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente, quando le accuracy sono così alte, le performance degli algoritmi di classificazione vengono espresse sotto forma di errore percentuale di classificazione, che può essere calcolato come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_error(gt, pred):\n",
    "    return (1-accuracy_score(gt,pred))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'errore percentuale di LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore LeNet su DIGITS: 2.11%\n"
     ]
    }
   ],
   "source": [
    "print (\"Errore LeNet su DIGITS: %0.2f%%\" % \\\n",
    "    perc_error(mnist_labels_test, lenet_mnist_predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 4**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confrontino le accuracy di training e test. Possiamo dire che il modello generalizza?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 4**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello di LeNet che abbiamo definito è una versione \"classica\" di una CNN. I progressi nel campo della ricerca hanno sottolineato che:\n",
    " * Il max pooling funziona meglio dell'average pooling;\n",
    " * Le ReLU sono più robuste delle attivazioni di tipo Tanh.\n",
    " \n",
    "Costruiamo una versione \"più moderna\" di LeNet modificando questi due elementi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetV2, self).__init__() \n",
    "        #Definiamo il primo livello. Dobbiamo effettuare una convoluzione 2D (ovvero su immagini)\n",
    "        #Utilizziamo il modulo Conv2d che prende in input:\n",
    "        #  - il numero di canali in input: 1 (si tratta di immagini in scala di grigio)\n",
    "        #  - il numero di canali in output: 6 (le mappe di feature)\n",
    "        #  - la dimensione del kernel: 5 (sta per \"5 X 5\")\n",
    "        self.C1 = nn.Conv2d(1, 6, 5)\n",
    "        #Definiamo il livello di subsampling. Questo viene implementato usando il modulo \"MaxPool2d\"\n",
    "        #Il modulo richiede in input la dimensione dei neighbourhood rispetto ai quali calcolare\n",
    "        # i valori massimi: 2\n",
    "        self.S2 = nn.MaxPool2d(2)\n",
    "        #Definiamo il livello C3 in maniera analoga a quanto fatto per il livello C1:\n",
    "        self.C3 = nn.Conv2d(6, 16, 5)\n",
    "        #Definiamo il successivo max pooling 2d\n",
    "        self.S4 = nn.MaxPool2d(2)\n",
    "        #Definiamo il primo layer FC\n",
    "        self.F5 = nn.Linear(256, 120)\n",
    "        #Definiamo il secondo layer FC\n",
    "        self.F6 = nn.Linear(120, 84)\n",
    "        #Definiamo il terzo layer FC\n",
    "        self.F7 = nn.Linear(84, 10)\n",
    "        \n",
    "        #Definiamo inoltre un modulo per calcolare l'attivazione ReLU\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.C1(x)\n",
    "        x = self.S2(x)\n",
    "        x = self.activation(x) #inseriamo le attivazioni ove opportuno\n",
    "        x = self.C3(x)\n",
    "        x = self.S4(x)\n",
    "        x = self.activation(x) #inseriamo le attivazioni ove opportuno\n",
    "        x = self.F5(x.view(x.shape[0],-1)) #dobbiamo effettuare un \"reshape\" del tensore\n",
    "        x = self.activation(x)\n",
    "        x = self.F6(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.F7(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 5**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti il codice di LeNet con quello di LeNetV2. Quali righe di codice sono cambiate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 5**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il nuovo modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_v2_mnist = LeNetV2()\n",
    "lenet_v2_mnist = train_classifier(lenet_v2_mnist, mnist_train_loader, mnist_test_loader, 'lenet_v2_mnist', epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/lenet_mnist_v2_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 6**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confrontino le curve di training e test di LeNetV2 con quelle di LeNetV1. Quale dei due modelli converge prima?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 6**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'errore di test e confrontiamolo con quello ottenuto in precedenza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore LeNet su DIGITS: 2.11%\n",
      "Errore LeNet v2 su DIGITS: 1.19%\n"
     ]
    }
   ],
   "source": [
    "lenet_v2_mnist_predictions_test, mnist_labels_test = test_classifier(lenet_v2_mnist, mnist_test_loader)\n",
    "print (\"Errore LeNet su DIGITS: %0.2f%%\" % \\\n",
    "    perc_error(mnist_labels_test, lenet_mnist_predictions_test))\n",
    "print (\"Errore LeNet v2 su DIGITS: %0.2f%%\" % \\\n",
    "    perc_error(mnist_labels_test, lenet_v2_mnist_predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Immagini Naturali e \"MiniAlexNet\"\n",
    "\n",
    "Consideriamo adesso due dataset più complessi: CIFAR-100 e CIFAR-10. Ciascuno dei due dataset consiste in $60000$ immagini a colori di dimensioni $32 \\times 32$. $50000$ immagini sono utilizzate per training, mentre le restanti $10000$ immagini sono utilizzate per test. Le immagini di CIFAR-100 sono suddivise in $100$ classi, mentre le immagini di CIFAR-10 sono suddivise in 10 classi. L'immagine che segue mostra alcuni esempi delle $10$ classi di CIFAR-10:\n",
    "\n",
    "<img src = \"img/cifar.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carichiamo il dataset CIFAR-100. Normalizzeremo le immagini utilizzando le medie e varianze per canale, che sono state pre-computate (si vedano gli scorsi laboratori per un esempio su come calcolare questi valori)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "cifar100_train = CIFAR100(root='cifar100',train=True, download=True, transform=transform)\n",
    "cifar100_test = CIFAR100(root='cifar100',train=False, download=True, transform=transform)\n",
    "cifar100_train_loader = DataLoader(cifar100_train, batch_size=1024, num_workers=2, shuffle=True)\n",
    "cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adattiamo il modello LeNetV2 per prendere in input immagini a 3 canali (RGB) di dimensione $32 \\times 32$ (invece di $28 \\times 28$). Aumenteremo il numero di feature maps e unità nei layer fully connected per aumentare la capacità della rete. Sostituiremo inoltre l'average Pooling con il MaxPooling e le attivazioni Tanh con le ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LeNetColor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetColor, self).__init__() \n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature maps\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 18, 5), #Input: 3 x 32 x 32. Ouput: 18 x 28 x 28\n",
    "            nn.MaxPool2d(2), #Input: 18 x 28 x 28. Output: 18 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(18, 28, 5), #Input 18 x 14 x 14. Output: 28 x 10 x 10\n",
    "            nn.MaxPool2d(2), #Input 28 x 10 x 10. Output: 28 x 5 x 5\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(700, 360), #Input: 28 * 5 * 5\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(360, 252),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(252, 100)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 7**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Confrontare il codice di LeNetColor con quello di LeNetV2. Le dimensioni delle mappe intermedie sono cambiate? Perchè?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 7**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_cifar100 = LeNetColor()\n",
    "lenet_cifar100 = train_classifier(lenet_cifar100, cifar100_train_loader, cifar100_test_loader, \\\n",
    "                                  'lenet_cifar100', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/lenet_cifar100_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 8**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Possiamo dire che la rete converge? Perchè? Come mai la differenza tra accuracy di training e accuracy di test aumenta all'aumentare del numero di epoche?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 8**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su CIFAR-100: 0.24\n"
     ]
    }
   ],
   "source": [
    "lenet_cifar100_test_predictions, cifar100_labels_test = test_classifier(lenet_cifar100, cifar100_test_loader)\n",
    "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,lenet_cifar100_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy è piuttosto bassa. Proviamo a migliorarla aumentando la capacità del modello. Definiamo un modello più \"profondo\" con 5 livelli di convoluzione e tre layer fully connected. Inseriremo il max pooling solo tra il primo e secondo livello, il secondo e il terzo, e il quinto e il sesto. Per evitare l'eccessiva riduzione di dimensioni delle mappe di features, specifichiamo un padding pari al ceil della dimensione del kernel fratto 2. Questo farà sì che le convoluzioni non riducano le dimensioni delle mappe di features. Utilizziamo kernel di dimensioni più grandi nei primi layer e più piccoli nei layer successivi. Il modello è vagamente ispirato al modello \"AlexNet\" proposto da Krizhevsky et al. nel 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MiniAlexNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNet, self).__init__() \n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature maps\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            #Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), #Input: 3 x 32 x 32. Ouput: 16 x 32 x 32\n",
    "            nn.MaxPool2d(2), #Input: 16 x 32 x 32. Output: 16 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2), #Input 16 x 16 x 16. Output: 32 x 16 x 16\n",
    "            nn.MaxPool2d(2), #Input: 32 x 16 x 16. Output: 32 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #Input 32 x 8 x 8. Output: 64 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #Input 64 x 8 x 8. Output: 128 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1), #Input 128 x 8 x 8. Output: 256 x 8 x 8\n",
    "            nn.MaxPool2d(2), #Input: 256 x 8 x 8. Output: 256 x 4 x 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            #FC6\n",
    "            nn.Linear(4096, 2048), #Input: 256 * 4 * 4\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_cifar100 = MiniAlexNet()\n",
    "mini_alexnet_cifar100 = train_classifier(mini_alexnet_cifar100, cifar100_train_loader, cifar100_test_loader, \\\n",
    "                                  'minialexnet_cifar100', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_cifar100_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test e confrontiamola con quella del precedente modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su CIFAR-100: 0.24\n",
      "Accuracy MiniAlexNet su CIFAR-100: 0.21\n"
     ]
    }
   ],
   "source": [
    "minialexnet_cifar100_test_predictions, cifar100_labels_test = test_classifier(mini_alexnet_cifar100, cifar100_test_loader)\n",
    "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,lenet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNet su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_cifar100_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 9**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Confrontare le curve di loss e accuracy di MiniAlexNet con quelle di LeNetColor. Quale dei due modelli soffre di più di overfitting. Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 9**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Regolarizzazione\n",
    "\n",
    "Negli esempi precedenti, abbiamo osservato una grossa differenza tra accuracy di training e accuracy di test. Questa differenza è dovuta al fenomeno dell'overfitting e si acuisce quando i modelli contengono molti parametri e i dataset sono piccoli. Oltre alla tecnica di regolarizzazione mediante weight decay, esistono altre tecniche per ridurre l'overfitting. Vedremo in particolare le seguenti tecniche:\n",
    " * Dropout;\n",
    " * Data augmentation;\n",
    " * Batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dropout\n",
    "\n",
    "Il dropout permette di ridurre l'overfitting rimuovendo in maniera casuale dei nodi dalla rete a training time. Per implementare il dropout, basta utilizzare il modulo apposito `Dropout`. Modifichiamo MiniAlexNet per introdurre il dropout nei layer fully connected. Posizioneremo i moduli dropout esattamente dove sono posizionali nell'architettura AlexNet. Come probabilità di dropout utilizzeremo $0.5$, che è la probabilità di default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MiniAlexNetV2(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNetV2, self).__init__() \n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature maps\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            #Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), #Input: 3 x 32 x 32. Ouput: 16 x 32 x 32\n",
    "            nn.MaxPool2d(2), #Input: 16 x 32 x 32. Output: 16 x 16 x 16\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2), #Input 16 x 16 x 16. Output: 32 x 16 x 16\n",
    "            nn.MaxPool2d(2), #Input: 32 x 16 x 16. Output: 32 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #Input 32 x 8 x 8. Output: 64 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #Input 64 x 8 x 8. Output: 128 x 8 x 8\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1), #Input 128 x 8 x 8. Output: 256 x 8 x 8\n",
    "            nn.MaxPool2d(2), #Input: 256 x 8 x 8. Output: 256 x 4 x 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(), #i layer di dropout vanno posizionati prima di FC6 e FC7\n",
    "            #FC6\n",
    "            nn.Linear(4096, 2048), #Input: 256 * 4 * 4\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            #FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello su CIFAR-100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_v2_cifar100 = MiniAlexNetV2()\n",
    "mini_alexnet_v2_cifar100 = train_classifier(mini_alexnet_v2_cifar100, cifar100_train_loader, cifar100_test_loader, \\\n",
    "                                  'minialexnet_v2_cifar100', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_v2_cifar100_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test e confrontiamo con i modelli precedenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su CIFAR-100: 0.24\n",
      "Accuracy MiniAlexNet su CIFAR-100: 0.21\n",
      "Accuracy MiniAlexNetV2 su CIFAR-100: 0.40\n"
     ]
    }
   ],
   "source": [
    "minialexnet_v2_cifar100_test_predictions, cifar100_labels_test = test_classifier(mini_alexnet_v2_cifar100, cifar100_test_loader)\n",
    "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,lenet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNet su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV2 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v2_cifar100_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 10**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Il dropout ha migliorato notevolmente l'accuracy di test. Si può anche notare una differenza qualitativa nel training dei due modelli (con e senza dropout). Si confrontino le curve di accuracy e loss del modello appena allenato e della rispettiva versione senza dropout. Quali sono le differenze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 10**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Augmentation\n",
    "\n",
    "Un altro modo per prevenire l'overfitting, consiste nell'\"aumentare\" i dati in maniera sintetica. Ciò permette di forzare il modello a generalizzare rispetto ad alcune condizioni. La tecnica della data augmentation viene applicata trasformando \"al volo\" i dati in input in maniera casuale e facendo in modo che, dopo la trasformazione, l'etichetta sia ancora valida. Alcune trasformazioni comuni sono le seguenti:\n",
    "\n",
    " * Flip orizzontale, con probabilità 0.5;\n",
    " * Color jittering: il valore di ogni pixel viene perturbato leggermente in maniera casuale;\n",
    " * Random crop: un crop di dimensione inferiore a quella di input viene estratto casualmente dall'immagine.\n",
    "\n",
    "Applicando la data augmentation, ad ogni epoca, il training set sarà \"leggermente diverso\". Ciò permette di ridurre l'overfitting. La data augmentation si applica specificando apposite trasformazioni. Le trasformazioni vengono applicate solo in fase di training.\n",
    "\n",
    "Vediamo di applicare le tre trasformazioni viste sopra. Estrarremo crop $28 \\times 28$ dalle immagini $32 \\times 32$ di CIFAR-10. Ciò ci costringerà ad adattare la struttura di MiniAlexNet per accettare in input immagini più piccole. Per compatibilità, in fase di test estrarremo crop $28 \\times 28$ dalla parte centrale dell'immagine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ColorJitter(),\n",
    "                                      transforms.RandomCrop(28),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "#in fase di test specifichiamo solo il crop centrale (non potremmo classificare immagini 32 x 32)\n",
    "transform_test = transforms.Compose([ transforms.CenterCrop(28),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "\n",
    "cifar100_train = CIFAR100(root='cifar100',train=True, download=True, transform=transform_train)\n",
    "cifar100_test = CIFAR100(root='cifar100',train=False, download=True, transform=transform_test)\n",
    "cifar100_train_loader = DataLoader(cifar100_train, batch_size=1024, num_workers=2, shuffle=True)\n",
    "cifar100_test_loader = DataLoader(cifar100_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per capire l'effetto della data augmentation, proviamo a caricare e mostrare a schermo la stessa immagine del training set più volte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAq8AAAD7CAYAAABE4X1VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOy9aZBc13Xn+b8v18raq1BYCyt3cDdJrZRkUaYsyY6Q+sPMtDw9oZ62R44OO9yO6HBYnv44ERP+Mo6YiVBPtGy56W532zFh2S3allubRVIUJZEgQRIEiJUoAFUooAq1ZlVWru/OB8J5zv8SVZUoFKpeos4vAoH3cHN5mXnePRfv/N/5O+89DMMwDMMwDKMdiDb7AAzDMAzDMAyjVWzxahiGYRiGYbQNtng1DMMwDMMw2gZbvBqGYRiGYRhtgy1eDcMwDMMwjLbBFq+GYRiGYRhG23BLi1fn3Oecc6ecc2edc19br4My7jwsVoxWsDgxWsVixWgFi5M7E7fWPq/OuRSA0wCeBTAK4DUAX/ben1i/wzPuBCxWjFawODFaxWLFaAWLkzuX9C0890MAznrv3wMA59xfAvgigGWDYnDbNr9//4Hmvl43O3cLR6KgpXiwMPeNenM7bjT4iU4uQqcy2WBsuTcAXPAPjVq1uV1ZWqSxanmpuV1aLPPrpOSnyHUWaCytjqezqys47JS8d/CZ4kZNtn2dxk6eOHXNez+EjeGmYqWzq8P3D/Q291Mp+YxRxCGbyXU0t9OpDI1lM7nm9s2El1t2B5iavNbcLhXnaGxo+7bmdq7AvyFceKqtU8DfZt48+npi4wQAugp5P9DX3dzPqN8829lLj81m82qPz9tr1yab2+XiPI2l1eQUqfPt/WOU14nSHH96Hil0d9NYXsdHME+VSyXarywUm9uZIGwidW6kO3luSKtzY93m1xXm7KNvJDdWwtyTJFbKWYhVPikt0NDU5Aztp/Odze2BoW005tR8o2MGACL1Q4aXsvR+rV6hsWpN7cece2p1yW9LS0s0NjE+k9g4AYDevj6/Y/fO5r7OKzqnAK3P4h94nPoHnVMAzis6pwBBXmnTnHIzrJR/bmXxugfAJbU/CuDDKz1h//4DePEnR5r7+qqvW6fZNVbbPuaTrT4jQVKe44nAZ+XE796zm19ULWydj2koinlRWBy/0NweeefnNDby7jvN7dePnKWxbO9gc/ueJx+jsf7dw83tj378E/y8Qk9ze3qWk25l4Upze6E8QWMfeexTF7Bx3FSs9A/04rd/739u7vf29De3O/J8Mu+568Hm9kDfMI3t37W/uZ2OOL7oPx38kyLSjw3i8r/+yTeb26//6B9o7F//zr9sbh965Eka85lB2o9St3LqbRy9hSixcQIAA33d+L2vfqm5v2P3Xc3tg099nh47vP8+tcc/+n/8k//Q3D754vdobFAtSnN5XiBmUhJH3dt30Fj3rr3N7cc+/Ys09uAjTzS3GzWeQ86+dYT2z/z4xeb2jiwfd0H9J2/7Ex+nsW13P9TcDhfda2WlObunwyU2VsLckyRowdjgixpYko949shPaOg//4e/pv2hw/Lx/6f/7TdoLKVyWE8f/6cuk5X4joMFUFVdELk8dZHGxq6ckcNe4v/IX5081dx+59hxGvu//4+/TGycAMCO3Tvx9f/0x819nVd0TgE4r4QXsvQUEwX5R+cVnVMAzis6pwCcV9o1p9wMK+WfW9G83mi1+QENgnPuq865I865I9cmJ2/wFGMLsGqs6DhZXCjd4OHGFuCm55SFUvkGTzG2ADc1p1ju2bLc9JwyNzO7AYdl3Cq3sngdBbBX7Q8DuBw+yHv/De/9k977J7cNbVSlwEgYq8aKjpPOrqDkbmwVbnpO6Srkw2Fja3BTc4rlni3LTc8pvf19G3Zwxtq5levMrwG4xzl3EMAYgH8O4NfW5ahuBaW9mTp7koYmXpeyS2k6KHNUZR1/zyc+RWN3PyqX6qMMf2XvHD9G+2/+6IXmdnH8Eo0VJ642tzNp1tWWp8ab2y/8PZdn7v/UZ5vbH/nkZ2isUhFN1MwEX2E//5qUH65ePodN5KZipVav4sqkzC/bdopsYHCISyW1huipAskhlziD/6fpvVq9RmMzsxIbjUAmMjUuUoxOpUcDAL8kshTXKNKYywYTopODvfOUSmvmpueU6kIRF19+qbl/tU/Ox65u/s77uiWOXn71XRr75h9/q7m9O81Xc3c8IKXCXIov2izWZL4Zu8TatfNvy3n8zhTLlH57UKRJ5ct8vp//8Q/4/VMifxpIBfOGkgpNneJzfHD4HtkJNdjtz23LP5spPQtzhpaejbzL+SyKOBaK4xJHL36HJQUrSc+Q1tIzzotaelYOpGeoyxw3Mf0eDc3Myvfk403tyHnTceJ9vGxeCW9w13kl/JQ6r8wE36vOKzqnAJxXdE4BOK/ciTnlZhoIrHnx6r2vO+d+G8B3AaQA/Kn3/vgqTzO2IBYrRitYnBitYrFitILFyZ3LLSl8vfffAfCddToW4w7GYsVoBYsTo1UsVoxWsDi5M7njbk/zZSnzhGU0qBLbQIrv8EUkl+rPv8Rlu7SXi/D53fto7M//6u9o/8SRt5rbB/u5VDcQSYmxM8N3/zZUO47zp1mS85PTUtLcNXyYxp7+0APN7cmTP6Wxt7/335rblVluq5JovKNSky5BuaAd1q4ddze3y7NccptQsZDr2kljA31SKqtW+Qaxo0d/Ju99eZTGdGkuLNudPCplvXKZT60DD9EuutVdq3HQ/surEiOCEmNxTGLDVbkVW75X7oRP9wetciJp8XIn2eqlogjdHeqzNeQ7efn5P6fHvn5UpAI/fpNLoGOX5fyIBllH++5FKevdv587WtTUbzc5xy2BzlyQ541N82/1xL3SJaMww7KBfHGc9jvzUkqbnQ/kL9sONbeHtnOpOsoGLf/Wg3atR94Mmyg907IzgGUEWnYGbL70bHC/5MJSzPFdK8lnzEbb0Vb4BkkidF7ROQXgvKJzCsB5RecUgPOKzikA5xWdUwDOK2vOKQDlFZ1TAM4rOqcAnFd0TgE2Pq/cSXnMMAzDMAzDuMOxxathGIZhGIbRNtji1TAMwzAMw2gbNlfzehv0U1rn1RVowCZHR5rb5UnWMnYq55pimQ/s1M+UzqmfdZXf/94rtF8qilamK9pFY939ohFZrLCd3kmlq7uyyO0ixqZEj/cXzz1HY6Nvip5o6dLrNFZQ+r9sB+tTkoxzadJJ1UqiDy5FrN2dnROXv3MXWDu0kttSZ4dyWwraD717WtzPbsZt6d1johcbvcrtlt46xXo17bik3ZYAIFaOS7fDbQnr5LaUBOpxjOmKfNe9alIpz3CbmWNnRJ9+cZp1rV29Yt96eY61jNUlaVq+dx877PQo97f5C6dprDg1LY8LYuX0cXEgKiyM0Nj9nazBnp6Vz1cJHOYO3Ssa+B0PP0pjPi3T+4pTbdidZqUH68feofrXzbxvQt8zAfB9E/qeCWDz75tYHJPXKRz+BRordEicdh/sRzvRKJcxe/rGeUXnFIDzCuUUgPKKzikA55XBoMejzis6pwCcV9aaUwDOKzqnAJxXdE4BOK9QTgE2PK/YlVfDMAzDMAyjbbDFq2EYhmEYhtE2bK5soNUS1E2UtXSpLCyj1Rak/PfexVM0tjQtJcZqjkuKp09Li51SF7fDSdf44HSpcH6wk8by+++VsRkuwRxTbXWuVbmM3dUrl+4vnn2bxl6dljLCPdu4/JDJyLHNVVp3rthsstlO7Dv44eZ+oyLflZ/nUvDFE280t6fOn6CxpLotAey4pN2WAHZcMrellfEuhUYk58eEaieUC1rX5DqlHFe6NMWvo37KTJr/T7+wJOXgkbExGvutL31RvR+X9C+N/01zuyPN88Y59Rs/82Eu8e3ZxuW34mWRKh36hY/xYx+XMl4mmLcQqw8VBZOm/sDB1ODd8hOs2wKygc2UnmnZGcDSMy07AzZfelZZFHnL/i6O/WiHlNfL7ja0bLuNVBdLuPjKjfOKzikA5xWdUwDOKzqnAJxXdE4BOK/onAJwXllrTgE4r+icAnBe0TkF4LxCOQXY8LxiV14NwzAMwzCMtsEWr4ZhGIZhGEbbYItXwzAMwzAMo23YVM2r1k/5FfRTzq+g1wz0WU7pvEIN2J4PqfZBLA/F+BuiO9qzey+NTV0T3ck//vwojYVatsFu0Yt86hOsT/vQo9Ja5Otf//c0pnV14XF7ZVVXWuQ2Orm9g83t2LM+c2JC9Crp/h1oF1KZDHq2ie1e3g80t2PuDoLT6vvoDjRZSbUKBdguVFuFAmwXmmir0CSQysL37lO7okGbq7OWq7NPNOjdnYGdpTp1KnXWmVW9aFAnFnmspKw1v/Srz9LYwrzo33/w4k9obPqajD3w1EdpbN++Ado/+rq0LxoP5oay0urf0xlYOa6gXY2V0LURWBDXKtJWJ5flc0qrceO4fXT08Gj5HovNvG9C3zMB8H0T+p4JYPPvm+je3tHcHr/MOt6egsRwdoDjOenEcUx63uXspwHOKzqnAJxXdE4BOK/onAJwXqkFNq86r6w1pwCcV3ROATiv6JwCcF7ZbPtpu/JqGIZhGIZhtA22eDUMwzAMwzDahg2VDXjPpSa9co4DaUClKiW/TJoPM+XUZe3wOrMqldWDHjDnVFlnJijrVO8Vt4h9T3AZr3ZRSjl/9fc/5LElLiN86XOfku2gjHj27Pnmdlh+1KXJjOexbFrGuvN83J19Q83tuRofS+cOabPiO7htEHAEycXDpWvNveK0lFzmg/JUvUNKV8VrLJtIqtsSwI5L2m0JYMelTXdbSjiVah3nxiab+yX1ey1V+XzYvl2+rxSC829JynGVpRqNxapYXo25jdXsnJLmBK956IDIj4rPL9CYj8U5Z7E4S2PHxzmOv/0zaduT6+BS3YF9IpnofOazNLZnSEp86WAOLZXlPY4e5zJycUGO9WNPfZjGegsSt9VqFW3FcvKzBEnPtOwMYOmZlp0Bmy89c51yPr19nFvzzb0uc9quQwfRVji3bF7pDWZPnVd0TgE4r+icAnBe0TkF4LyicwrAeWWtOQXgvKJzCsB5RecUgPOKD+aUlvPKzbRFXQG78moYhmEYhmG0DbZ4NQzDMAzDMNoGW7wahmEYhmEYbcMGa149qjXR5eRUq4ViiTVhr7z28+Z2dxe3gHn8wUea210dbEnWaEibl8uTl2nsxZdFrzpykVtHVJReKLubtYyNomhCJi5coLGFIh+31rmFGjitjwu1c/WGtKuJS2wR6LyIq1J51kRNKT3oxATrOjuy0mal0Mttg5LM4vw8fvbd7zb3x98TrXBvnkN2307R52ibUCC5VqEA69zOBdZ92i50s61CE09cR7wo54BTbZ+6OzhWulTs5DI8VpyVtjP5HM833V2if1sscsub8XGZY0YvD9FY7zbRnO/dzy1n+rLyu3Y41iC+dow1kadPnmluD/UHltOxzD9vvclj3R9+Rt6vbzuNjarjPhvMafp+g+wbb9DYhx77heZ2rtCBdiGGR03dS5DU+yb0PRMA3zeh75kANv++idFJyVONJdZNVpWt6AvPv4O2IpUhPe9y9tMA55Vc0KpO55VQbq3zis4pAOcVnVPefz85rrXmFIDzSjG4j0TnFZ1TgCCvhK3ydF4JPzC1RW3RfnoV7MqrYRiGYRiG0TbY4tUwDMMwDMNoGzbWYcsBTpUvdEuWI29yeeqiKmuFLi9DA3Lp/N4Dd9HY/Lxcqn/rTXa1uTJyQrYvcol9ckbKJW8d+ymNPTV8X3P74E4uDc4G7iG6VDh6mUsMusQYlh97VWmytMCygeKslJxy29nVSZdCl4IyaaMuEgpdWk06xdlZvPD83zb3D+6Rlj+NvrB0JdsF5bQEJNdtCWDHJe22BLDj0ma7LSUe5xGlpLVVqiGfxQVlLR/J77O7n1vXXMrI99UIyqpdHSLbmZuapLGzx8RV5+FhPjfv3i1x+5tf/mc01tcrcfzwgQM0duzCm7Q/dv5kcztd48ceOijvee4Sl5WRkRg7dOABGvrOd34sr1lgacruvXJsr77BEoa6ct/65MeeRruwWCrhZ0elPWBSpWdadgaw9EzLzoDNl5516Tk0zcfSkZZzb++2Php7NwjTpFGtNUgSUVjGwQ/gvKJzCsB5pcaqCsorOqcAnFd0TgE4r6w1pwCcV3ROATiv6JwCcF5pNacAnFe0gx/A6zv+JlbGrrwahmEYhmEYbYMtXg3DMAzDMIy2wRavhmEYhmEYRtuwsa2yYqBRET3HKz9/tbn9xvFj9NhD9+9pbl++xFqf5//uH5vbX/gCWzm+NyIatFADFqVEyzEzwe2RxkZHmtv5xpM0pjVpv/mv/gWNhTqkQ0qTOX6Z9VJaH1cMtHO9g2LR16iz5q6gZIihVk/r+FzMesVUpKx4UyuatyWKbDpFOimtn8qkWGdaLMr3Pz7L2tWkWoUCbBeqrUIBtgvdbKvQpOM90IhFQ1Wrye8VRezfmSnL47rzfB4dulcsLEemWOe1L5b/4z9x7yM09lC3aODzr/Ic5rdJC5pP/PIzNBb3i+1jvcS6tu39rHP+1GfEknHk3ASNLdUkzhauBnPa1Reb21PzQYuvCXkdH/PcMDgkGrR0jueUH70kms/HH3kY7UKlWqWWYEm9b0LfMwHwfRP6nglg8++baESq9WP/II0hLXHZ09s+LdUAII4by+aVUmAXrvOKzikA5xWdUwDOK3Gg9NR5JVxf6Lyy1pwCcF7ROQXgvKJzCsB5RecUgPOKzikA5xV9rxPAFtTafno1Vr3y6pz7U+fchHPuHfVvA8657zvnzlz/u3+l1zC2BhYrRitYnBitYrFitILFydajFdnAcwA+F/zb1wD80Ht/D4AfXt83jOdgsWKsznOwODFa4zlYrBir8xwsTrYUq8oGvPcvOecOBP/8RQC/eH37zwC8AOD3V3utOG6guCCX3XUJanA3X3KvlKUFxcX3rtCYU+Xw195+hcaOq8vTLvh4Kb2f5rYfujQXlu10We/B++6lsWiGSymj3xVJQ/4aX6r/JVVi3BmUH1+fHG9un+zgcueBYSklDQUOU+WylId0qxQAiGPVQijNr3k7WK9YSaUiKjV5XX7xXH5pqBJfHLSuSqrbEsCOS9ptCWDHpc12W7odrOec4uDhVJw3VE+aVBDyXv1DnOLvfFe/lMqHytzm5ZODUh4b3sGt2px6v/lJLvFlFyRWF0e45JvNSxxVwfKPpQpLVXbuFomDD5xrpqdFflQLyv/9AyI/Grt8isbGxqQcfWWUS3zXZs41twe3ccl3x6DIuTKd3MLwdrBesVKvNzAzI+dHUqVnh4JWgFp6pmVnwOZLz+pKolNa4FyXSktMa1nP7WI95xTfqKExL/lY5xUXtBLUeaUryM06r+icAnBe0TkF4LwSSkV0XllrTgE4r+icAnBe0TkF4LyicwrAeWU0OG6dV7SDH8AuftrBbzXWesPWDu/9OABc/3v7Ko83ti4WK0YrWJwYrWKxYrSCxckdzG3vNuCc+6pz7ohz7sj09NTqTzC2JDpOKrXa6k8wtiw6VqqBiYRh/BM6TsIbmAxDw/nn9l8pNm6dtS5erzrndgHA9b8nlnug9/4b3vsnvfdPDgwMLvcw486lpVjRcZLL3H6Jg5E41jSnZNM348li3CHc9JxS6Oq+0UOMO5s1zSmhjMxIJmv9lZ4H8BUAf3j972+38iQXOdJJ9Q6I7mNs7Bw99u23mjcN4uJZbnO0c1j0PIM7WWcWK33NzDTrNTJKv7P/EGsSta4s1JxVy6IJaixxW5ulQMtWGhGtzPwc64A6lJ7pyX17aGxnTt6/e4pfM600KHGG/1foG7LQ09o/gPV/7vbL05bjpmPFw6MB0cWk1H+xSgus3amrzxgF65ikWoUCbBeqrUIB1sdttlXoBrKmOcXDw6tz3tfld60FTrephmj2uot8HhfmZI4Z3nWAxpxqAfPcEdbYz5UlHnsjjpXPP/h4czv95nEau+eQvIfrZ83bZKCVHxuVlkzDu++hsYlJOU9GLo7RWDojX0BvH/+HcGFRXvPSGMdxHEn8b9/B7bAaysry1aOswdxAbjpWqpUa3TuR1Psm9D0TAN83oe+ZADb/volcWum2AzvarLLcjTKblnzWNqf4eNm8onMKwHlF5xSA84rOKQDnla7gt9J5JdQ567yy1pwCcF7ROQXgvKJzChDklQy3kdN5RecUgPOKtp8G2IK6Xmn9qncrrbL+AsBPAdznnBt1zv063g+GZ51zZwA8e33f2OJYrBitYHFitIrFitEKFidbj1a6DXx5maHPrPOxGG2OxYrRChYnRqtYrBitYHGy9dhQccdiqUylJl2CSqX4UEbeG2luj41x+b+rX8onjQb3HS4WpYw3O81ygwOqVL99KGgXNCrtIvrSXLbLPCjlkfQct2q6FJQDT8zL+//DCR6bi6UU0Zsv0Niz9z3R3P5olp0zRq+ONLdTvVxiqBekFFqrcMsbH1fVdhvpeJxHFEn5qlGT8lx1iWUD6bSUpBoV/m2S6rYEsOOSdlsCuMS42W5Licd7xOp3rqs2LJk0l/jyU/K9Ds8FN3oV5BwfqfF8c2JMyrGV7l4a69wvDjRdHVwOO9+Q4xqe599q8fLF5nbvLv6NH32I2ywN7JB5K53m8mNfr5RrC518bKm0/JZPPP44jXXmJMbPn/tLGnOQc2/7ILcw9JDv9PzIO2gXSqUlvPGayAGSKj3TsjOApWcdQRutTZeeKScoF/P5VFe5KBvquZKOc4iURELnlVpwM7HOKzqnAJxXdE4BOK/onAJwXnkokIrovLLWnAJwXtE5BeC8onMKwHlF5xSA84rOKQDnFe3gB7CLn26fuhq3vduAYRiGYRiGYawXtng1DMMwDMMw2gZbvBqGYRiGYRhtw4YKISvVJdJJpZUmLdRWad1VvoM1M88889nm9v2HWUvSqEjbhe0DrHkb3iX6tKEB1jke3Htfc3vvENuu6VZNc0qrBgDTgZbtPEQT0/UIayLrS6Ktmpueo7G/vSBa4MPb+f0PaLHRFdZ1LvWKJsXXuXVLXbUNimtt1DvVA2iIfkjrp0JtFWJtbbi8XitJVqEA24Vqq1CA7UI32yo08XgPr4wKnNIrRo6nN6/a2qQibgk0n5Kxi2n+P/39n/5kc/vwfdw6SreZ7QlsHvMqHqOgddnopdHmdmkPaxe3DfGcdve9okk7N8L2vSMXft7cfvAwt9HKd0ocVauseUvn5Tv75V9lS8bBQfkcew7wZ8pk5DU7ujimk0y1WsGlMfkNknrfhL5nAuD7JvQ9E8Dm3zeh5+VGg+eplLJRjdNtdL8FAHi/bF7ROQXgvOKDJKPzis4pAOcVnVMAzisueD+dV9aaUwDOKzqnAJxXdE4BOK/onAJwXtE5BeC8ou2nAbag1vbTq2FXXg3DMAzDMIy2wRavhmEYhmEYRtuwodfzs9kYuw/Ipe7+bVL2qNW4rPXZX5F2MVNTfHlcl7zCcthjjx9ubpcXeezyRWlP8egDh2nsrgP7m9uz17jkO35FLsdPq3IfAER376f9pz8tJb9y0J5pXl3mDy3ZT5ySFhgXT/Fl9SFV0uwJyp26FBo5HnPkPNRGHvA+RqzaY+lt3TYL4DYv2mkJSK7bEsCOS9ptCWDHpc12W0o6zjmk1W+SUtuNQGJSSsnvXOrppLF5ZUm8lGEZychVcWaaWeTXrKk2OoHaAB97SuawnX1cGrxaljnmxZe/R2OVHMfO7j1SRpxfYCcjLVty01yq627IdxGl+LgzBTnuuw7z591/l7TROXOeHd4unZRYSbeRNW8UOZKfJVV6pmVnAEvPtOwM2HzpWUO1TVsqc0uxfKe8ZrZj8+wd14T3JIlYzsEP4LyicwrAeUXnFIDzigtkFTqv6JwCcF5Za04BOK/onAJwXtE5BeC8onMKwHlF5xSA84p28APYxU+3T10Nu/JqGIZhGIZhtA22eDUMwzAMwzDaBlu8GoZhGIZhGG3Dhmpei6U5/Pi1/97crysd5t4D3Crr0Y+JJvXCuSs0FjnRnU4vsA1m3BDNRHGONZDT86IZeu0t1gudOif6pctjrC3KKf3L/blBPpZO1rJdUW1QXnntxzRWVxKRTI41KHMLqs1RoLmbz4uWJB20dSpBtSsJNH5a/5duq3YlHoBqlVUT3U95kfVaHXnRz8Q1/r2TahUKsF3oYqCB03ahm20VmnScc8hkpQ1MLif6umpg5bjkJY4Wstw6xiu7xLkyx1hUk/1cjn/XfF5eJwo01z946UfN7Qfv4Xni6afvb24P1dmuN0oF7YoactyZiN9jQFkrzpbO0thiXX7znm7WZ9ZUC556oOObX5DzZvcw6xzTaYmrTJq/wyTT01uglmBJvW9C3zMB8H0T80FrpM2+b6KhptTwXEsr/We93j4t1YD3W0U1qvJ5dF7ROQXgvKJzChDklQLne51XdE4BOK/onAJwXllrTgE4r+icAnBe0TkF4LyicwrAeUXnFIDzim6DCnCbVG0/vRp25dUwDMMwDMNoG2zxahiGYRiGYbQNtng1DMMwDMMw2oYNFULmcmkculu0FzWltdq+M+yJKjaIC4usCUsrrVWtwT0R54ui0ajVWT8xMCzaikyONa+pvPRh23c/r+njhux3p1k79vLL79L+8TOibeoONJEukq+7HGhnpmflM8Y+sLVUeryFGf4ulqqih3OONShZpevLZNtHn+a9J51UvSH6vFBblVUaQN9gTVZSrUIBtgsdDTRw2i50s61C//TEz5BkvPdo1OW7TamGmrmIYz52qu+v51jp2y2a1D1V1pXOjY80t4e3s5VorOawUpG1qg2llU0XeN7IDfY1t3sbrHnrynM8Tk3LOT8e9HLNdorGd/9O1uNmssp+e4g1b1DasuIC6zNPKUvS/j7+vA8flr6jnUGMJ5lch6N+tkm9b+JKYB2r75uoB72qN/u+iVpdz688T6bUZFipcK5LOj72aFRvnFdcoGvXeUXnFIDzis4pAOcVnVMAzithK2WdV9aaUwDOKzqnAJxXdE4BOK/onAJwXtE5BeC8olHPCNAAACAASURBVO2nAbag1vbTq2FXXg3DMAzDMIy2wRavhmEYhmEYRtuwobKBQiGPJx4TC71F1frjxIm36bEzqox+3+EHaay7S5fHuFQ+MSmX52tVHivOiqSgGNiXDQ7sUNtcKlsoq1Jkqo/GwnKgLhVmHVtQFrpkPwrkB3OTcpm/dxdbzvZl5Weanz5NY7oUmsvxJfdIyQh0aTXpeB9TqUmXoMLyVF2VdDJBWSupVqEA24Vqq1CA7UI32yo06Xgfo9KQ449VG5aOrgI9Np1Wn7vCc0OszvnZ09xm6NqYfK8XMqeC15TvKyzHPvuZZ5rbew8eorFdw2Lf2Si9R2OZjsBmsl8+x+59XP7X54YLSpPlWOLq6hTHw7E35T1HL/JcWK/Kd/jQAyx98qqF3fwCPy/JxHENC4ty7iZVeqZlZwBLz7TsDNh86dlSWeQG6SzL/qJIPm89biNrcgCAXzavpIIco/OKzikA5xWdUwDOKzqnAJxXdE4BOK+sNacAnFd0TgE4r+icAnBe0TkF4LyicwrAeUXbTwNsQa3tp1fDrrwahmEYhmEYbYMtXg3DMAzDMIy2wRavhmEYhmEYRtuwoZrXuFEnnVQEafNSnOM+ICdPStuPs++9RGPD+0R39PBjrCXbp8byEWs5fENrQFmHk81IuwbH8hR0LInuaVeB3++xx7jtw2Cv6DleeekVGpubEY1UPXj/SaWr852sCYnvvUt2GqxJ0i0pculAV7MoWqa4wS0+koz3QKy0RGmln0oH2qG6amlSKLDGMalWoQDbhWqrUIDtQjfbKjTpeAfETutcVduVPP+uGXXOBdJp9O8RLXv/NdZATqndhcXZZY9F6/wA4OoVOac/8slforGhHWLBODXBxzlT4jY3nT1K91gLrFyVPnN2lnVu+Q458NmpRRpbWpDz4YH7+J6Cg/tFc79ziM+pKC2x4qL2ufaRy6dw1wOia07qfRP6ngmA75vQ90wAm3/fRE3FYmcHx0lD9fUqdLFuGki2Vvr9Vo03zis6pwCcV3ROATiv6JwCcF6Jgt9c55V8MIfpvLLWnAJwXtE5BeC8onMKwHlF5xSA84rOKQDnFW0/DbAFtbafXo32mX0MwzAMwzCMLY8tXg3DMAzDMIy2YUNlA84BHVlZL/tYLkl/7CPs7HPoLrnsff4COwlNqPJIWA7LZ+Sy/tWlSRrr65PL8V1B6dRn5BL4wjy3QOnvHG5uD20forHiXpYNHPmpOBJNz3J5JI4DixSFU6XJ/gEuW+qS5mLw342Mqn9mOoI2R07KX0tLXJpIMlEUoVCQsurSgvweYTuixZqUPMKyVlLdlgAuFWq3JYAdlzbbbSnpOOdIHpAryPmY6+Bzsw9K1lHjc2yueLG5vXeAp8XxXuXGhoVlj6Vc5t/8nRMiG3pq+i4a6y3L77NY4/ZI1Ygdt+plKVXPzfF7XBlXc4znc2NvYW9z+9GH2Q3u8Ucfa27n8/w9pahUzJ93qSz7cRu1QJqbW8Df/53Iz5IqPdOyM4ClZ1p2Bmy+9Cybl/k1BZ6nqkvymh18qiUe7z1JIpZz8AM4r+icAnBe0TkF4LyicwrAeSUOZFw6r6w1pwCcV3ROATiv6JwCcF7ROQUI8wqP6byiHfwAdvHTDn6rYVdeDcMwDMMwjLZh1cWrc26vc+5Hzrl3nXPHnXP/5vq/Dzjnvu+cO3P97/7VXsu4c7E4MVrFYsVoFYsVoxUsTrYerVx5rQP4t977BwB8BMBvOecOA/gagB967+8B8MPr+8bWxeLEaBWLFaNVLFaMVrA42WKsqnn13o8DGL++XXTOvQtgD4AvAvjF6w/7MwAvAPj9lV7LOSBKidYjyoguoqOXtTaDO/c0t+9/iPUiZdVmItRdXbkm2o6JuSkam1BWZzt3baOx3l4R5sQR67wWa7LGnyq/RmOXp1kjqHVulTJrXvP55cU/nUpXF2rutB4v6uPX6MvI54jB+hiy6PO3V5+2nnECAMqFErUl2ckGmletu9I2oUByrUIBtgvVVqEA24VutlXo7WA9YyWVSqG7T+m7lF6zp4/brgwqHWB1kc+V4ydebW7fu20fjT37iUeb29lAZzY3I62zskHLtYeeEv2WK5yksR+8IpaI49dYt9+zg8/xHUMSqynw2LZeOf97ulmP39ctsdJVCOxKIe1q0mn+TDOq/VaxzMdWqcrcODe7vP53vVivWMnn8tQSLKn3Teh7JgC+b2KleyaAjb9vwkcyVg9aA+pD3Yj7LdZ1nRK5ZfNKHGh7dV6hnAJQXomD1mk6r+icAnBe0TkF4Lyy1pwCcF7ROQXgvJIK3l/nFZ1TAM4rOqcAnFe0/TTAFtTafno1bkrz6pw7AOBxAD8HsON6wPxT4Gy/mdcy7lwsToxWsVgxWsVixWgFi5OtQcuLV+dcF4BvAfhd733LtyQ7577qnDvinDsyN9M+d7wba2M94qRaa5+7mI21sx6xUqmtfDXKuDNYS6zoOCkFjdGNO5P1mFNqDb/6E4xNp6VWWc65DN4PiP/ivf/r6/981Tm3y3s/7pzbBWDiRs/13n8DwDcAYN99vf7MZblc3tsn7ZByVY6z7ry4ifQH5Zm8uqQfgUt1Q/2Dze1MmlvAzBelBJTyXI6dn5Uyz9VJlhvMX5Wy/dltb9PYcO9jtP9r/+MnmtvvvMaPrValVNnbz60sqqrNip/lktPxE8ea2weGumhMl0Lri9M0NqVam3Rn+P2Aaaw36xUnPYWM16WmOFblKcdlBV260k5LQHLdlgB2XNJuSwA7Lm2229LfgMvd68V6xcpgb4fXny2bl5Lvth0sN+osqHLYIpe899TlnOsbCiRM2+V5+RIntninSAwuT3DLq8WSxMebb/D3OFO+JMfVxReDimd5oXVwu8xpPd1BC7RBifF0mmPVqTLmzAK3yikt6RjgWK3GMjZd5DiqVCTmyuWNcWNba6zoOLn/0b3+87/y0eZYUqVnWnYGsPRsJdkZsPHSs4WifMZUFMgNVEupoDPXbWO95pTufOSXyys6pwCcVzJB6zKdV3ROATivTAU/a6t5Za05BeC8onMKwHlF5xSA80o+6IGm84rOKQDnFe3gB7CLn3bwW41Wug04AN8E8K73/o/U0PMAvnJ9+ysAvt3yuxp3HBYnRqtYrBitYrFitILFydajlSuvHwfwvwA45pz7p7sM/ncAfwjg/3PO/TqAiwD+h9tziEabYHFitIrFitEqFitGK1icbDFa6TbwMoDlLvx/Zn0Px2hXLE6MVrFYMVrFYsVoBYuTrceG2sM2GjFmlU1YuS76hlyOtVU1pe1aWAhbsogGpdDRSSOdBWkRkc+y7mNbr7xmLWh5M6+0XWNnWbuWjuRrOnb1Eo2NBnqVe7Jia9sf6NN2bxcNXhS0FikX5LybyrCWZQ9E89uRZn1KvlM+47YSH0ytIbqTarm9blggnZTST4WtSmpKr9bVyXqkpFqFAmwXqq1CAbYL3Wyr0P8T30KSSaXTGBgUzWj/NqUP7ed2MU7p3PMdgY6+IudV/0GeU/bfL6/53vGzNNZIica2bz//jlFBYjU3zq/Zr+apg3ftpbHw5qLBgZ3N7WyW9bjTU6LJXAh0rfluec9ihS0gL12QtjY93azd7O6T2OnM7aCxjrR8pnRvqDp7GUnFoY50WnT+Sb1vQt8zAfB9E/qeCWDz75vwqi3dwhxrI/MFib2OHn4/4CqSjMfyeUXnFIDzSl8QRzqv6JwCcF7ROQVoPa+sNacAnFd0TgE4r1BOASiv6JwCcF7ROQXgvKLtpwG2oNb206th9rCGYRiGYRhG22CLV8MwDMMwDKNt2FDZQDaTw/AOubRdr0sJKkrxOnppSUokE7Pc2me+KJey9+7nslZ3Ti7dl4v8vK4uKV8MDA7SWCYj7RoO7ucSW6FLSoPnz3E5NpfmcmC0Sz5T3w6+dL+wIJfnUw0uDR568O7mdnySW27U6lJ+yOfYDaMRyfsNhC5SGTnWmWtcxgJOIKlEqRSVmhYaEgtLJS6Vd/VK+be7lyUVSXVbAthxSbstAey4tNluS0knilLIKQlAvlN+5ygo+XrllJOK+LvryMrz8oMcD6ld8rzdvTxvlGvy3eV7+f2mp2WeemQPl9jSqQPN7dA5qdRVov0l5WqVzvOc4pSKII74daZnZB6LMhyPnQWZN1OOv4tuNY+k0pwiFpekrFyp8HEmmWqthkvjWlaRTOmZlp0BLD3TsjNg86Vntaq8f2mBv4tcTt6vv38nmHNIMlEUobNnmbwSlLx1XtE5BeC8onMKwHlF5xSA84rOKQDnlbXmFIDzis4pAOcVnVMAzis6pwCcV3ROATivzATtt7SLn3bwWw278moYhmEYhmG0DbZ4NQzDMAzDMNoGW7wahmEYhmEYbcOGal5jH6NaF51ULifais4ObsPRqCs9zRxrqzoLouVs1FjLNV0SnVc+yx+P9WGs89C6su07uY1OoSDajp07WddSb/DrVGLRdgwMsF6kPCdj+QxrqVIFNTYZ6PGuiO4pillL0oDoRaIUa+60/i+3uDFWjutBKpUmnVStpHRXFf78HUqvFtrVJdUqFGC7UG0VCrBd6GZbhSYdD4+al/golkRPVQ3ahaVUG8ia4/O2sE3mEdfL2tErSnMf1TkecnllI1nic8xVRZ9dBmsCo0jitlHjOFpcYk3YgtrvWuC5SdtFujS/TkNbuZZYH9nbLTGmrVIBYHpWYrfhee6NUvJdV8ob5Pu5DlRrMUbH5HdM6n0T+p4JgO+b0PdMAJt/34SL5TMtLbEdaConY1FqQ5cZt0wUpZbNKzqnAJxXdE4BOK/onAJwXtE5BeC8onMKwHllrTkF4LyicwrAeUXnFIDzis4pAOcVzimAzivafhpgC2ptP70a7ZWpDMMwDMMwjC2NLV4NwzAMwzCMtmFjZQNxHYslabVSj+XyeHGBW3uknJQvnOunsd5uucxeKvHzMqq1i0tzeWZRuTcUL7OjF5VkuHIDH0t5LJXhUlkcXAKPVGkyLnGpIJ2SF14sccmnWJWSjOvl1i2uU8p6i9f4snrNy6X7RtCeoqLKjbq0mnScc1Rq0iWoQhe3eenplpJH/yCPJdVtCWDHJe22BLDj0ma7LSWd2NdRqsnnzqjSeRQHsqGGzCm1LJ+35YycK5NBCdQ5eZ1C0AKtPyu/ebnE5fdSXX67uVk+b+dnJB5mp/l5lSqX6g/ctUceO8+lYy0biD1LA3S58/ix0zTW0ymv2TfApeLajMypmeDzRimZb7IpLkUmGYcIKSfnQFKlZ1p2BrD0TMvOgM2XnmXyEsMNP0lj3sk5tFTnXJt0UqnUsnlF5xSA84oLHNd0XtE5BeC8onMKwHlF5xSA88pacwrAeUXnFIDzis4pAOcVnVMAzis6pwCcV7SDH8AuftrBbzXsyqthGIZhGIbRNtji1TAMwzAMw2gbbPFqGIZhGIZhtA0bqnn1cQq1JdFsLCqda9xgvVatKnrRTKAtmj0vmrD5xTEae/Dhe5vbc1emaSxS2rXQkhFK1zpyjl8zlxW9Ut8Aa4J6+nn939un9FOBdi1fkOfOBVrGkmqz45dYr1jOiAalBtZSxTXRNtVS/H61tGiNSjX+LpJM7Bukk9L6qUbQ/iiTl+8jF2pXE2oVCrBdqLYKBTg2N9sqNOnUGzVMKx1Wdr/8XqUJ1rVuT8vnbAQfs5KX73V6apTGUtPyPTeqfL5rXWlpkTXuC6ptV63Gz6tXRcs2PsaawLvvZovQNCQey2X+TEs1Oe4YYcsrOZ50mmPl8hXRKF4c5edpzdueXdyqp1qTzxRFrL9NOk5Zeyb1vgl9zwTA903oeyaAzb9vYmxc8uSVCc4vQ7tl/vNBm7ak41xq2byicwrAeUXnFIDzis4pAOcVnVMAzis6pwCcV9aaUwDOK0uBJavOK45vq6C8onMKwHlF5xSA80p30I5NW1Br++nVsCuvhmEYhmEYRttgi1fDMAzDMAyjbdhQ2UCt1sD4qJQhYlVKyWa4zDE2LiWQapUvJafVpfq+/qAcNi4tGlIRl2AiyPM6gjYj+ay8fzrH5ZFTZ8W5YleZ3y99jS/rZzJyWb2rELRLUSWH8lKZxlJZ3XaEy1Gd+WEZi4Lr+Op1ZurcnsJtl+96eoFbXiSZRlzHfEnKmqWKlFUmJ7jEklatinqGuByRVLclgB2XtNsSwI5Lm+22lHQaMTBflN9oe1a+g0rEJfaGKntWFvl3nR+VeIt7+XnZlMwVnVluc7RYke95aYllJHPTEnOdPXydYEi139m75xCNdXfx79qRkv0ouN5QKql4DObQxUUp6x3Yf4DGLsQyH7z7DssdurqVO5LnseKCvKZvo2sfjUYdM3NSZk2q9IxkZwBJz7TsDNh86dl7F8XFqTjHc2jfsMRQPWofd0cAqNZqJInQeUXnFIDzSiqQfOi8onMKwHlF5xSA84rOKQDnlbXmFIDzykKQY3Re0TkF4LzSCNywdF7ROQXgvKId/AB28dMOfqvRPrOPYRiGYRiGseWxxathGIZhGIbRNtji1TAMwzAMw2gbnPd+9Uet15s5NwngAoBtAK6t8vCNYqsey37v/dAGvddNYXHSEht1PImNE8BipQVsTkFi4wRI1vHYnILExspWPZZlY2VDF6/NN3XuiPf+yQ1/4xtgx5JckvR9JOlYgOQdz2aTpO/DjiW5JO37SNLxJOlYkkCSvg87lg9isgHDMAzDMAyjbbDFq2EYhmEYhtE2bNbi9Rub9L43wo4luSTp+0jSsQDJO57NJknfhx1Lckna95Gk40nSsSSBJH0fdiwBm6J5NQzDMAzDMIy1YLIBwzAMwzAMo23Y0MWrc+5zzrlTzrmzzrmvbeR7X3//P3XOTTjn3lH/NuCc+75z7sz1v/s36Fj2Oud+5Jx71zl33Dn3bzbzeJLGZsaKxUn7YHMKHYvFygrYnNJ8X4uTFbA5hY4lsbGyYYtX51wKwNcBfB7AYQBfds4d3qj3v85zAD4X/NvXAPzQe38PgB9e398I6gD+rff+AQAfAfBb17+PzTqexJCAWHkOFieJJwFxAlistAUJiJXnYHGSeBIQJ4DFSmt47zfkD4CPAviu2v8DAH+wUe+v3vcAgHfU/ikAu65v7wJwaqOP6fp7fxvAs0k5ns38k4RYsThJ/p8kxInFSnv8SUKsWJwk/08S4sRipbU/Gykb2APgktofvf5vm80O7/04AFz/e/tGH4Bz7gCAxwH8PAnHkwCSGCub/rtYnHyAJMYJkIDfxmLlAyQxVjb9d7E4+QBJjBMgAb9N0mJlIxev7gb/tuVbHTjnugB8C8Dveu/nN/t4EoLFSoDFyQ2xOLkBFis3xGIlwOLkhlic3IAkxspGLl5HAexV+8MALm/g+y/HVefcLgC4/vfERr2xcy6D9wPiv3jv/3qzjydBJDFWLE6SRxLjBLBYSSJJjBWLk+SRxDgBLFY+wEYuXl8DcI9z7qBzLgvgnwN4fgPffzmeB/CV69tfwfuajtuOc84B+CaAd733f7TZx5MwkhgrFifJI4lxAlisJJEkxorFSfJIYpwAFisfZIPFvl8AcBrAOQD/bhPExn8BYBxADe//D+vXAQzi/bvlzlz/e2CDjuVpvF+OeBvAm9f/fGGzjidpfzYzVixO2uePzSkWK+0QKxYn7fPH5pT2iBVz2DIMwzAMwzDaBnPYMgzDMAzDMNoGW7wahmEYhmEYbcMtLV4320bNaB8sVoxWsDgxWsVixWgFi5M7kzVrXq/bqJ3G+24Lo3j/Lr0ve+9PrN/hGXcCFitGK1icGK1isWK0gsXJnUv6Fp77IQBnvffvAYBz7i8BfBHAskHRVcj7gb7u5n4mk2tuZzt76bHZbF7t8QL72rXJ5na5yP1y0056DEdRisack9eJ0hkaS2Wyze1CdzeN5QsFdSh8LOVSifYrC8XmdiZodxyl5HjSnV183LkOdZxYF/Shhq959I3Xr3nvh9bnnVblpmKlt6/P79i9s7mfTslvlVUxA9y4o/SN+MDj1D9MTV6joVJxrrk9tH0bjeV0LLjw9FmnHy5BvHk0uXECAIPbtvn9+w9szNFtFMEcU6tWmtuLizzfdPX0NLfT6VuZztXbq+04uLbRaNSb29VKmcbOnDqV2FgZ3LbN79t3YIMO7VYIvnAv33clyDWTEzxvFbolhw4O8bylX/ZmLlfpx1ZrFRqrN2rN7VplicbiWI670WjQ2Nilq4mNE2Dtc0p4HbCuvpP64gKNxeo7qQXPy3XJ+oPWHgAl8nDtUSrK2qNRq/L71Wu07728Thzz71NXHyTf3UNj27bpn43zXbUq80F1cY7Gaip20tksjdURy3EHE87lFWLlVma7G9mofXilJwz0deP3vvql5v6O3Xc1tw8+9Xl67PD++9ReTGP/8U/+Q3P75Ivfo7FBtSjN5XmBmEnJF9O9fQeNde+SvsSPffoXaezBR55objdqdRo7+9YR2j/z4xeb2zuyfNyFAZlctj/xcRrbdvdDze1w0b1W9FV1F6xeezrchXV5k9a4qVjZsXsnvv6f/ri5P9A33Nzev2s/PTYdyedy4bSsvv4oChaW6vv4r3/yTRp6/Uf/0Nz+17/zL2ns0CNPNrd9ZpDGotT6LB6SRG8hSmycAMD+/Qfw4k+OrPSQ9qPBi4SrF881t1999SiNPf1Lv9zc7h8MFixrfXu1vRQsPIoL083t9869S2Of//inEhsr+/YdwIs/ee22H9StEjc4v7jaVHP7vbc5zv/f/+c52n/i05JDf+03fp1fWOWCOPwfiRIP+mBBUlePvTDOP+/07Ghze+zccRpbKsvCem5+hsb+4Hf+KLFxAqx9TgkXgdfOvtPcnnj9JzRWmpbF3dUqqzfv+cSnmtt3P/okjaUykmOOv/06jb35oxea28XxSzRWnLhK+7WG/M6VMi+sp9RC9/5PfZbG/tff+E21x8c9euFUc/v8a/9AY1cvyxw2uH8fv1+8KMe5wPH/7373/1o2Vm5F89qSjZpz7qvOuSPOuSMLpfINnmJsAVaNFR0nczOzG3RYRsK46Tnl2uTkDZ5ibAFuak6ZumZxskWxOeUO5VYWry3ZqHnvv+G9f9J7/2RXIR8OG1uDVWNFx0lvf9+GHpyRGG56Ttk2tFHVRyNh3NScMrjN4mSLYnPKHcqt1DmbNmoAxvC+jdqvrfSE6kIRF19+qbl/te9Yc7urmxcsfd39ze2XX+Xy1Df/+FvN7d1pvpq74wEpK+dS/B+sxZpc1h+7xHqh829fbG6/M8WX0X97cHdzu3z5Io2d//EP+P1TUvIbSLG2ozwr+typU+dobHD4HtkJdS7tz03Fivcxag3RC2l5cniDoVP//wr/J1ZT5Y+ZWdbgNLxoCqbGr9BYZyx6Ib/EceIaoity2WCR7eRA21X9usmmJTc9p3i/6cd8y3ywVMxl1uLEe83tF57/Gx4ryvx3O0rFl8d5vtOl4iuXNvWel5uKFec+KJ1KCvqXiRyXnqHmm3Au0vMUwPPYxBUuE6ec/OA9fXx/SSYr81YcxIJX82RwmwjN0YM7WEJ1Vd1HcOXcB9aKG8lNzylrxZdZ7kM5fpbvzRlIqXM+4t/x/Euypkh7/j3yu6Xk/ud/9Xc0duLIW83tg/28hhiIOK46MyJNbKT4hz1/Wn6vn5z+Fo3tGj7c3H76Qw/Q2OTJnza33/7ef6OxyqzMaYtjh2mscPgXZLujdenTmhev3vu6c+63AXwXQArAn3rvj6/yNGMLYrFitILFidEqFitGK1ic3Lnc0h0m3vvvAPjOOh2LcQdjsWK0gsWJ0SoWK0YrWJzcmWzo7dGpKEJ3h7Q6ihpyl9nLz/85Pfb1oyIV+PGbEzQ2dlkuQUeDrKN996KUTu7fP0xjtUg+7uQct/Y4c0GeNza9SGNP3Ptgc7sww2W0fHGc9jvzUgSanedCdrTtUHN7aPtuHgvaR6wLyaySrY5vAHUpl5Vn5c7JiaA0k+uSlloDfdzWo1qVViJHj/6MxmYuS/mzGJRGo0h+i5NHT9JYuSwxdOAhGkK36oQQR3xqeReIGlQ5rjjGZTVXlfjL9wYt1fqlrOIibhu2Fe3yklwOXolWS8UAl4uTXCq+U9A9YnwctIeakd+iPMfyMp/tbG537+H5Heq3cJ670ESqrVR4l/jIOz+X7Xd5LtLz1PvPlXnsxe/8NY3175Zc+NGPf4KPLS3z5nQgr6osSHyVy5yH9Rw9Mf0eDc3Myvfk460xM4U5vEvl+MnRERorT0r+6Qy6EhXLcj6e+lnQpaBf4uP733uFx1SrrK5oF41193OuWKzInHPyIsvmrizK7DQ2xRKmv3juueb26JvbaWzpknQ/KDR4DZVV675K0O5vf5fktGjHXWiVrRFVhmEYhmEYxh2BLV4NwzAMwzCMtsEWr4ZhGIZhGEbbsKGa13ocY1pZCvYqrVV5htuAHDsjbSAuTrOutatX7NMuz7FGp7okDe737mM3pp4eab81f+E0jRWnxDmmJ3DmOn1cnCMKCyM0dn8n6zemZ+XzVfLc9uHQvdIiYsfDj9KYV9aOKyr4wq5AKz1YP7aNZIGNchmzp6UNz7kLouXSrmwAO7N1dtxHY1Ctyt49fZaGtDPbYCDs085s7x5jg4/Rq/L7vnWKNYbamU27sgFAvIIzm3ZlA9iZTbuyAezMpl3ZAADr5My2FUmqzhFgrWOSdY53DMopaeos60y1U5J2SQLYKUm7JAHslBRlOO2+c1xaRmqXJIBjI3RJyqSDVoxTcv/FC3/POn7tlPSRT36GxioV0VHPTPB8p52StEsSwE5JpZg1jrWSfMZsxNrItqPFPOoDe2ad42sLbLzz3kVZUyxN89qnmpP1zunT3Ca01KVaSAa+snoNMz/YSWP5/ffS/vyMaFmPXWDN67WqxFVXL+efi2ffbm6/Os1tSu/ZJnk0k+Fjm6tod9MOGhu/LDHet7Ad7QAAIABJREFUUxhAq9iVV8MwDMMwDKNtsMWrYRiGYRiG0TZsqGzAuxQakVyGnlBlkFzQ5ijXKaXb0qUpfh11RTqT5vX3wpKUQEbGxmjst770RfV+XNK/NC7ONR1pbqN1TrlqPfNhLgfv2cal2qK6BH7oFz7Gj31cSr6ZXGCVqx1woqA2oT9wIBvwK7QJcm0qG6gulnDxlTea+1PnRUKgXdkAdmbTrmwAO7NpVzaAndm0KxvAzmzalQ1gZzbtygawM5t2ZQNWdmbTrmwAO7OVA2cW7dpCrmzAnejMtnEktFQMcLk4yaXiOwXtlBQ6IWqnJHJJAsgpSbskAeyUpF2SAHZK0i5JADslreSSBLBTknZJAtgpSbskAeyUpF2SAHZK0i5JADslaZckgJ2Sug/yvJx0PAKlwEoOfir/usDFTuf4PR/6OHhQNsff4JZXe3aLm+3UNf7N//HnR5vb4TplsFvmhk99gtceH3qUf/Ovf/3fN7f1mik8bl/ntn0l1eYqt5db5cVecurEBOetdP+O5rbr5LXX28flHJt7/RRaxa68GoZhGIZhGG2DLV4NwzAMwzCMtsEWr4ZhGIZhGEbbsKGaV6Sy8L371K7oFefqrPvr7FMtaDqDNhyqQ0OlzpqQqhcd0MQij5WUzutLv/osjS3MS5uJH7zIlmzT12Tsgac+SmP79nFrh6Ovi2ZoPNC1llVLjHs6A9vPFbSrsVLgNIKWO7WK6K5yWbaA04qoOF5Bt5Mw4jgmC7nlLIUBthXWlsIA2wprS2GAbYXfDezxtK1wLbB51bbCZ4IWI9pWWFsKAyvbCmtLYYBthbWlMMC2wmYpvH4kVecIsNYxyTrHOwV9XnUFNt7a5lNbfAJs86ktPgG2+dQWnwDbfGqLT4BtPley+ATY5lNbfAJs86ktPgG2+dQWnwDbfGqLT4BtPrXFJ8A2n2V3G+ap20gceyyplp4Z1QIrFdh8R3rCDHJ4XeXtc0E7rBm1Nqjeyy0P9z0ha4zaxWka+6u//6GMLXEu/NLnRHMfrm/Onj1P+3ptpNdMAJDxMpZN81h3Xo67s2+IxuZqcjydO9ie1nfIPU2jkxzjjSX5rqvBPR4rYVdeDcMwDMMwjLbBFq+GYRiGYRhG27ChsoFKtY5zY5PN/ZJyx1qq8iXw7dulDJFCIA1YktJtZalGY7EqlldjvuQ9OyeXpNPBax46IO0pis8HLjqxtJJZLLJTxvFxdpn49s+kxVOug8slB/ZJ6bDzmc/S2J4hKU+lA6eOUlne4+jxt2msuCDH+rGnPkxjvQWRJlSr3A4j0TiHeoe4cBSv3diVDWBnNu3KBrAzm3ZlA9iZTbuyAezMpl3ZAHZm044mADuzaVc2YGVnNu3KBrAzm3ZlA9i1JXR0admZrVVXti1EUkvFAJeLk1wqTjQf6H+0wkPVeRU6IWqnJO2SBLBTUjWQjGmnJO2SBLBTUjinaKeklVySAHZK0i5JADslaZckgJ2StEsSwE5J2iUJYKck7ZIEsFNSdqB116QksLBYxAs/fbm5390l8/rjDz5Cj+3qEIlPo8GSosuTIuN58eUf0tjIRZGRVYJWVdndkn8aRc4NExdkLbJQ5HWKXsOE6xu99gF4bVRvsBQxLsl85DzHQyovcTU1zfE3MSHx35Flh69Cr8iyuvp4rFtJEzrSrScgu/JqGIZhGIZhtA22eDUMwzAMwzDaBlu8GoZhGIZhGG3DxrbKiuuIF0Un4VTbp+4OPpSuvOznAmvF4qxoe/I5bjnV3SU6nMUiazLGx0WDMnqZ2zz0bhOd2d793J6oL6s0Ga5EY68dO0r7p0+eaW4P9bO2Ix+LRuWtNwPdx4efkffr205jo+q4z15gK8dKVbQk2TfeoLEPPSatbHKFDrQNqQxZyC1nKQywrXAuaD+mbYVDhz9tKxza42lbYW0p/P77yXFpS2GA7frOBXawK9kKFwO9mLYV1pbCQGArHLY/07bC4QfWDsOtWgq3O+FnaUOdI8BaxyTrHBPPcjafwfmgbT5DG2+y+eSvjWw+tcUnwDaf2uIT4HlDW3wCbPO5ksUnwPNYeNza5lNbfAJs86ktPgG2+dQWnwDbfGqLT4BtPncdOoh2olKt4aLKuboF5dAA67zvPSAtwebn2cb+rTdF835l5ASNXbko88bkDN/v89YxaV331PB9NHZwp6xbZgMtsV7DjF7mPKnXPgCvjXq7eG1QWpBYKc7y3JTbLm0k9RoNAJbUGq5RZ/2vXvc1okB/269sZtOs1V0Ju/JqGIZhGIZhtA22eDUMwzAMwzDaho2VDTiPKCWtrVINKc+4oATqlXPN7n5uc3QpI+XhRp7LI10dUsuZm5qksbPHpIz38PAwjd29W9rj/OaX/xmN9fVKafrhAwdo7NiFN2l/7PzJ5na6xo89dFDe89wldrxARkoMhw48QEPf+c6P5TUL3P5r9145tlff4HJUXblvffJjT6NdqNYa5MJRWMaVDWBnts6wBYdyZqtxNYyc2UKHEe0+ol3ZAHYu0a5sADuzaVc2YGVnNu3KBrAzWzlwZtHObK26sgHszKZd2QAui/E3kXzCDkgat1yZGGibUjHA5eIkl4qTTAyPmnIO0k5JUagnUbFRD6JLOyXNBN+3dkrSLkkAOyVplySAnZK0SxLA881KLkkAz2PaJQlgp6TuIGdqpyTtkgSwU5J2SQLYKUm7JAHslPTC8++gnUinI/QNSCn98iX5LM//3T/SY7/wBVnPvDfCDo86x0cp/s5nJkRiMKZa8QFAvvFkcztcb/zmv/oXze2w/dUhJaEbv8wyAb32AYCiWhv1Dg7SWKMux1rgLlq0FtNrNABwsTw4FfF5E6XUOVXj9qYlJctKpVt3Y7Mrr4ZhGIZhGEbbYItXwzAMwzAMo22wxathGIZhGIbRNmyo5tV7oBGL3q6mtA9RFLRrKcvjuvOseT10r7TeGJliTeC+WNbjT9zLVm4PdYu2J//qMT62bdL25RO//AyNxf1iEVovsc5jez+3q/jUZ6TNzsi5CRpbqokOaeEqt9UYu/pic3tqPmjxNSGv42PWZw0OiV4xnWOByo9eEm3V4488jHYhjhsoFkXPMz4rOixtKQywrbC2FAbYVlhbCgNsKxwHSk9tnRfqirTtnrbjA9hWWFsKAyvbCmtLYYBthbWlMMC2wtpSGGBbYW0pDLCtsLYUBthWWFsKtwUeiJW2VbeOywT2ue2ocwRY65hknWOSWSyV8LOjR5r72uZTW3wCbPOpLT4BtvnUFp8A23xqi0+AbT4ngnaH2uYznFP0fLOSxSfANp/a4hNgm09t8Qmwzae2+ATY5lNbfAJs86ktPgG2+dy7je9TeDe43SNpxN6jUpbPevE9aUfnAi3na2+L5v14YN3u1PIqFS610vL6es0A8JoiXG88eJ+0zYuCtnmj3xU9bv4a55tf6ubWoDvV2uj1yXEaO6nuGzowzFbVQ6o9VrnMMUbxF7OuNZWW18yluTVXVcVqNjgXV2LVK6/OuT91zk04595R/zbgnPu+c+7M9b/7V3oNY2tgsWK0gsWJ0SoWK0YrWJxsPVqRDTwH4HPBv30NwA+99/cA+OH1fcN4DhYrxuo8B4sTozWeg8WKsTrPweJkS7GqbMB7/5Jz7kDwz18E8IvXt/8MwAsAfn+113LwcOpyckP1L0oFLWi8+oc4xSWJXf3K8aLMl/E/OSil1OEdXPJy6v3mJ7kEk12QsvLiCJeKsnm5zF0Fl1yWKnx5fOdu1UoiaNUzPS3tKWpB+b9/QMp6Y5fZxWdsTNwyroxyqe7ajLSrGdzGl+N3DO5pbmc6c7jdrFes+EYNjXkpZcSqVKpd2QB2ZgsdP7Qzm3ZlA9iZrTtwGNHuI6EziXZm044mADuzaVc2YGVnNu3KBrAzm3ZlA9iZTbuyAezMNhoct3Zm06V1gJ3ZtCvb7WI955RGHKNYku/oldd+3tzu7mIJRDuWigEuFye5VHw7WK9YqVSrdA5opyTtkgSwU5J2SQLYKUm7JAHslKRdkgB2StIuSQA7JYVzinZKWsklCWCnJO2SBLBTknZJAnjeXAqcLrVTknZJAtgpiVySAHJK6um9/e6O6zmnzM/N4/v/XUrwF8/K77pzmOU3gzvlO4hjbkE4My3neCaQG+w/JDGg1wwArymqZZYNNNR8sxSsU0ojkjPn51g20NHHa6En98naYGeO3797Sl43HbiExhn5jL7Bi7bl1nYA4PTyI5jDXCyxUq+0LkVa6w1bO7z34wBw/e/tqzze2LpYrBitYHFitIrFitEKFid3MLe924Bz7qvOuSPOuSPVemP1JxhbEh0nlVp99ScYWxYdK9PBDZuG8U/oOFmYn1/9CcaWhdYp5fa5EXErs9bF61Xn3C4AuP73xHIP9N5/w3v/pPf+yewGlJmMxNFSrOg40eV+Y8uwpjllYHDbcg8z7lxuek7p6um50UOMO5u1rVOCzhxGMlnrKuF5AF8B8IfX//52K0/y8PBKF+Lrot+oBTZkqYZoQruLrPsozIkGZXjXARpzqj3Oc0deobG5sugOeyMO0M8/+HhzO/3mcRq755C8h+tn/c5k0JJibFSuBA3vvofGJiZFPzZycYzG0hn5Anr7WEuysCiveWmMLW/jSHR923dwO6yGaqPz6lG2h9tAbjpWvI9RV5qZSP2fR1sKA2zrGdrVaSs7bSkMsK2wthQG2FY4tNXTtsLaUhhgW2FtKQysbCusLYUBthXWlsJAYCucYT2ethXWlsIA2wprS2GAbYXrlU276r2mOaVUWsQRpdm9qHSB2vYWaE+dI8Bax0TrHDeOm46Ver2BmRnRIGqbT23xCbDNZ2jjrW0+tcUnwDaf2uIT4PNfW3wCrGk+FGgTtc3nShafANt8aotPgG0+Q7t1PW9qi0+AbT61xSfANp/a4hNgm89G3F5zSrVSxch7I839sTGJm65+Po8bDWlgUCzyfQ2z0zI3HFAaUwDYPqTuTxjlex760vJdZh7k9UZ6Tu7NuRSsU07My/v/wwkem4v5anJvXjT/z973BI19NCsa/NGrIzSW6pVcWS9wPNSUXtXHnIt9LHNMqGttNCSOUsE9LSvRSqusvwDwUwD3OedGnXO/jveD4Vnn3BkAz17fN7Y4FitGK1icGK1isWK0gsXJ1qOVbgNfXmboM+t8LEabY7FitILFidEqFitGK1icbD02VlzoPWJVaqiTGw6Xg/NTctl5eC640asgl9JHatxK6MSYtIuodPfSWOd+cSvqCpxjzqtL18PzLI1ZvCztcXp3saPWow9xeWhgh5QH0mkuVff1Slmv0MnHlkrLJfgnHn+cxjpz4ih2/txf0piDXGbfHuj/vHIKOj/yDtoG5xApR45GRUol2pUNYGc27coGsDObdmUD2JlNu7IB7Mz2UOBMop3ZtCsbwM5s2pUNWNmZLXRY0c5s2pUNYGc27coGsDObdmUD2JlNu7IB7MymXdnagcVSiWQPh+6X8+/yJb5Jpx1LxQCXi5NcKk4y1UptWack7ZIEsFOSC1IkOSWluY2YPo9D50V9/muXJICdkrRLEsBOSSu5JAHslHQykEJpp6ShoKWgdkrSrdcAdkrSLkkAOyVVgzZt2ikpytz+No3ri6e8mu8QydUzyuEQAO4/LHmlUTlKY9sHJMaGd7FT4tCAnI8H995HY3uH5LdKBbXxObUWmQ7WKechv1XXIxwb9SWeC+emxanyby+wHOXwdnn/Ay747a5ILl7q5dzk63I+1Ouc7+KayueBg+FSWdZw+c7Wb+q/7d0GDMMwDMMwDGO9sMWrYRiGYRiG0TbY4tUwDMMwDMNoGzZc8+qVUYFTLTQix4fiVQukVMQ6nPmUjF1M8/r7/k9/srl9+D5uHaXbzPYElqB5pe2JAs3b6KXR5nZpD7e82DbEWsq77xXd07kRtoQcuSDWlQ8e5jZa+U5pLVKtsl4knZfv7Jd/le07Bwflc+w5wJ8pk5HX7OhaQtvgPRBrTd7ytnPaVtgHHsPaVlhbCgNsK6wthQG2FXbB+2lbYW0pDLCtsLYUBla2FQ7tAbWtsLYUBthWWFsKA2wrrC2FAbYV1pbCANsKa0vhdsA51uxWyqK70hpHoD11jgBrHZOsc0wypdIS3nhNfmNt86ktPgG2+dQWnwDbfGqLT4DP49A2XNt8aotPgG0+tcUnwDafK1l8AmzzqS0+Abb51BafANt86rkW4Pk2lD9qm09t8QlwO6Rs1F793Xt6O/HZXxH9+tSUzPM6FwOcqx97/DCNlRdl7HLQYu/RB+Sxdx1gW+nZaxKP41f4d5xWa5Hobn7e05+WtUg54vN2PshV2i/qxKljNHbxlOSHoRTrU3vUWkyv0QAgcjLmgvZoet0XdLtEVeno0w3W2K+EXXk1DMMwDMMw2gZbvBqGYRiGYRhtw4bKBpxzSCsHrJTabgRlh1JKLrmXejppbD4jl8SXMlySGLkqpcKZRX7Nmmq5FKgN8LGnpEyws4/LyFfLchn/xZe/R2OVHJdud++R0s584HijW1u4aS7rdjfku4hSfNyZghz3XYf58+6/S0qVZ86/SWOXTkqpIt1O1rzeU9lpOVc2gJ3ZUkHJQTuzaVc2gJ3ZtCsbwM5s2pUNYGc27coGsDObdmUDVnZm065sADuzaVc2gJ3ZtCsbwM5s2pUNYGc27coGsDObdmVrB6q1CsbGpMz19lvSEu7iWf7N27FUDHC5OMml4iRTrVZwaUzkYNopSbskAeyUpF2SAHZK0i5JADslaZckgJ2StEsSwE5J2iUJYKeklVySAHZK0i5JADslaZckgJ2SaoH7kXZK0i5JAEsDtEsSwE5Jcbq97L4LXSk8+jHJqxfOyZoicqP02OkFaZ0XN3juLM7JOTc9z/PNa29Jq6pT51g2dnlMHpsLfo/7c9IaL+rkdcoVFVevvMYOi/XAuCqTk3icWwikaWpNNZ/nWEkrKV4JgVOWmg9SwW+u1321Os9FkZPFWOom1il25dUwDMMwDMNoG2zxahiGYRiGYbQNtng1DMMwDMMw2oYN17xmstIyKJcTQVU1sP1c8qL9WchymyGvWtLMlVk/FNVkP5fj1iL5vLxOFLRy+MFLP2puP3gPa0mefvr+5vZQfYbGolSgEWrIcWcifo8B1dJntnSWxhbrojvq6WYNTE1p0ELbtfkF0UTuHg70KWmxoM2k+TtMMt57NKoSD3FNvkdtKQywrbC2FAYCW+ECa061rbC2FAbYVlhbCgNsK3w+0HlpW2FtKQysbCusLYUBthXWlsIA2wprS2GAbYW1pTDAtsLa+hBgW2EfWPclHeccUimZxkbeG2luj42xdrUddY4Aax2TrHNMMlHklrX51BafANt8aotPgG0+tcUnwDaf2uITYJvPuWBu0PdCaItPgG0+V7L4BNjmU1t8AoHN5xWORW3zqS0+Ac432uITYJtPbfEJsM1ntqO97GEbcRXzC9LmcmFRcn46yKO1hujo54s8V9fq8v0MDLN1eyYnv10qz/PNvvslWOIGX1/sTkvMvfwy27oePyOa9+5utp93UdAqT+XR6Vle08ReHuuD9n8LqsXfUpXnLedkTskGaza97lsq83yTzkpcRVHr11PtyqthGIZhGIbRNtji1TAMwzAMw2gbbPFqGIZhGIZhtA0bqnn13qNRV9aDSgiUi1gjETvR2tQ89xPs2y2a1D1V1pXOjY80t4e3s64tVvqdUjHoUaa0sukCa5lyg33N7d4G6yO78qwfnJoWTch40Ms12ynan/07WY+byYo+ZvsQ60ygtEXFBdY9nVL6uP4+/rwPHxa9VGdgh5tkfOzRUL+rb9zYUhhgW+HQrk7bCs8HNnfaVlhbCgNsKxy2ndO2wvmgP6a2FdaWwsDKtsLaUhhgW2FtKQywrbC2FAbYqjC0MdS2wtpSGGBbYW0p3A6kU2nS7Go9r9Y4Au2pcwRY65hknWOS6ekt0Dmgz4/QjlvbfGqLT4BtPrXFJ8A2n9riE2Cbz+lgbtA2n9riE2Cbz5UsPgG2+dQWnwDbfPYEdut63tQWnwDPtz54Q23zGd6zom0+6/U2siYHUCkv4fw5yav3HX6wud3d1RM8Wj7nxCTPG7WqjBVnWQ9bVH24Bwd20NjggOTxhTJfX8ylZC0SrlP0GibruDd+oYv3I6WdnZvkeOzdJfHYl+Ul4vz06ea2XqMBQC6n7ilyfD+GXvfVavy8zo6CelzQkHYF7MqrYRiGYRiG0TbY4tUwDMMwDMNoGzZYNhCj0pBL27Eq8XV0cQuYdFqVKCp8CTpWl9VnT3N55NqYlOMuZE4FryllRG1zBgDPfuaZ5vbeg4dobNewlOMapfdoLNMRWJL2y+fYvY/L/9r6zAVl7HIsZaarUyw3OPamvOfoRbb9rFflO3zoAW6P4SEln/kFfl6y8agrqUBmGUthgC3ptKUwwLbC2lIYYFthbSkMsK2wthQG2FZYWwoDbCusLYWBlW2FtaUwwLbCurwMsK2wthQG2FZYWwoDbCusLYUBthXWlsLtQDYXkezhs78iv8nUVPDbtWGpGOBycZJLxUkm1+HoHNA2n9riE2CbT23xCbDNp7b4BNjmU1t8AmzzqS0+Abb5vBK0VNM2nytZfAJs81kLbNO1zWeY+7TNZ2jTrufbdDD3aptPbfH5/vPkPSoVlqUknXK5jpMn5bs8+95Lze3hfdzy6uHHZK2wLxjLRzKv+0ZYRletxDL8Ozp1+ncs8TphV0He77HH+HmDvTKvv/LSKzQ2N8OxWlfvPznGOcZ3yuvE995FY1CfI5Sm5dJy4EuL3EYrbkisZPNBrKh1YHWpdctpu/JqGIZhGIZhtA22eDUMwzAMwzDaBlu8GoZhGIZhGG3DxmpeHRA7rXNVLXrygZ2Y0mQEchr075F2Ef3X8jQ2pXYXFtmuURPakF29IrqPj3zyl2hsaIfYdU5N8HHOlFjL1tkjulMXtITQ9nGzs6yJynfIgc9OsV3c0oJoyx6470EaO7hftHM7h1g3HKVFy+RuwnZts/Heo670VIWCfC5tKQyw7k5bCgNsKxza3GlbYW0p/P57iFYpH8SlthXWlsIA2wr//+2dy49b133Hv4eXnOG8H3pbViS/Uj/rOHAAJ21WRYAkDdpuWrQFCi8CdJNFAmTjtP9AV9l1EyCFswgKFEiAGO2iSI0ARdAmsREEthX5IdmWLWkkWRrNg8PhDIc8Xcx0fr/v0QyHtkecc6XvByCGnEvyHt77u+ce3vPj7+OVwkBvrbBXCgOsFfZKYYC1wl4pDLBW2CuFAS5/5JXCAGuFvVK4DLS7TVxvWc7u01+ynNSLFziXuYx5jgDnOuac55gz3W4bjRWLB6/59IpPgPtpr/gEWPPpFZ8Aaz694hNgzadXfAKs+fSKT4A1n70UnwBrPr3iE+D+zys+AdZ8hqTEkdd81hLlp9d8esUnwOfXjW7/eYw5MD09jT/7xje2H7930UoXXk/KSvlzdb3G56Zrq3ZsTk/z7xrGXV8da7zNG0sWVzNj99OyI0dNcb18ivuCV//3V9v35xf4twvd7u4lqAKHP2Zm7R9+rAUAKy6sa8nxX/OlCQMfN6ur1sfFCi/bcOfUHs28jfL0PkIIIYQQ4p5Hg1chhBBCCFEaBpo2EEKg9IDhUbvsPTzCl8Cn4WwNbb6uvbhstppTs/wR5qbsknSB3Q0wrRZPq77xeyst8YV5Lg8x1bJL/ittntZZr7Bxa6NlU06Li7yOq3PuUn7kabxTo6e27z/91FO07JmnP7d9v17n7VS4aZ7OBn9eb8DplmjqJsZIFo7drGwAm9lS44c3s3krG8BmNm9lA9jM5q1sAJvZOkm6gTeeeCsb0NvM5q1sAJvZvJUNYDObt7IBqZmNl3kzm7eyAWxm81a2MlAbCpT2sNSwKb7GCm/XMk4VAzxdnPNUcc4M1ws89JjFOZuS+PN7U5K3JAFsSvKWJIBNSd6SBLApyVuSAO430j7Fm5J6WZIANiV5SxLApiRvSQK43/SWJIBNSb4fBtiU5C1Jm8+1/m10PE1Fyrsc38hIDU88ZUbER5+0c0erxfvHn1ev3uA+/vqipSZdT0oeHj9hfcrUFPdF3Yqdt1fa3KfcbL2yff/KPKcp+THMWou3cb2e5AY4xqa4v/NjKj/WAoDKtL3PdI1Lg3Vh8ZCmZW64c3FjmccpRaXwD3ZtZ8qezwwhnAoh/CKEcC6EcDaE8O2t/8+GEH4eQnhn6+/MXu8l7l4UJ6JfFCuiXxQroh8UJ/ce/QxzNwB8N8b4GIDnAHwrhPA4gBcAvBxjfATAy1uPxb2L4kT0i2JF9ItiRfSD4uQeY8/Ba4xxLsb42637ywDOATgJ4M8B/GjraT8C8Bd3qpEifxQnol8UK6JfFCuiHxQn9x4fK+c1hHAGwDMAfg3gWIxxDtgMnBDC0b1eXxQFJqZdvo/Lp5mc5ryYQ05Rtp7oGs/+/jfb9z97+DO07Ctffnr7/lCSP7R4y0pnDSW5XE9+wXL9wuibtOy//sdK8czd4DJWk8c4l+TYEcvtKMDLDk9ZjsjkxBFaNj1hOUPjo0nuHCzPrVrlz3TL5eott7hta+uWW7K4sHv+737zaeMkVAIp5HZTCgOsFSalMEBa4W6Sg+a1wjcSPZ7XCnulMMAlh7xSGGCtsFcKA721wl4pDLBWuEjW77XCXikMsFbYK4UB1gp7pTDAWmGvFB4EnzZWOu0N3Lxin+2WywH9g8e5rFwZ8xwBznXMOc/xTvNpYmVxsYH/+PedNZ9e8Qmw5tMrPgHWfHYSda7XfIYkHdhrPr3iE2DNp1d8Aqz57KX4BFjz6RWfQKL5TFSlXvPpFZ8Aaz694hNgzWeR9Mte8zmye7rlHeHT9ikb3XUsNN/ffjzjylrVE7Vpxf0258gMl8OrVW2/Li1/RMuKaPtgaYH367WPLFd26RrnnJ4//Nr2/funPkfTJZ/7AAARS0lEQVTL/vavvrx9/41XXqNlqQ57asb6o/VETxtde87+/nVaduaIlQ09lMTYxsr89v2bSaxM1Gx9Mcmxbyxa/1of5b6vF31nx4YQxgH8BMB3YoxLez3fve7vQwivhhBeXWt/jCJeopTsR5y0O3HvF4jSsx+xsrK0vvcLROn5JLFC557mYL+UiYNhP/qUpfnVvV8gDpy+Bq8hhBo2A+LHMcafbv37WgjhxNbyEwCu7/TaGOMPYozPxhifHa6pMtfdzH7FSa0IOz1F3EXsV6yMTQ7t9BRxF/FJY4XOPaMDLawjDoD96lMmZ0d2eorIjD2P6LBZR+WHAM7FGL/vFr0E4HkA/7T192d7vVelUiGT1FDdygAdPsaljMZG3dTpCk95n9ywy9rTR3ia49BRe129yVfwusctxeDKdS5Ps9K0lILf/ZbTBm61PrR2jfOsw/J5Ll3zwFGbOpic4Cmn2iE7KKpVnksJbsr7VoNLbjRXfToAfwFY79qy+WX+orm2ZlelWq07e4VqP+MEiGTh2M3KBrCZrZZMo3kBSGoK8Wa2m8m0Vr9mNm9lA9jM5q1sQG8zm7eyAWxm8yWcADaz1ZP5OG978VY2gM1s3soGsJnNW9nuFPsZK51OxPKixcebb9r03Pl3/5ueW8apYoCni3OeKr4T7Fes1IfrdAx4U1JqNPSmJG9JAtiU5C1JAJuSvCUJYFOStyQBbEryliSATUm9LEkAm5K8JQng/m8luYbkTUlkSQLIlOQtSQCbkrwlabOtdj993Z1gf/uUNuaXLAWr0fDjD94HoyM2zT02yqli9SE7Bxye4j6l7VKFlpLz9uXzNjapJla116/ZWORSct56ZMisjjPJ2OO+ozy+8qbI1ij3DTdr1qecBMf4iDMw1sf4HHe4aQ1qd/j8s96ycVJ7nWOl2bBtMTzM6wNuYTf6+Tr6RwD+DsDrIYT/T/78B2wGw7+FEL4J4AMAf9nHe4m7F8WJ6BfFiugXxYroB8XJPcaeg9cY4y+R/rrB+JP9bY4oK4oT0S+KFdEvihXRD4qTew8loQohhBBCiNIw0Cz2olrF7CHLGZ057PJDZzhfJLhSEvURzoOor1nexcwDXFrh9KP2nu+ePU/LOoXlMk2f5pyQyqjlsgzP8XvOuFyWBx46RcuaDc55PTR7fPv+UKJPnL9pJTAaSV5rfcLWubzGeR4fXrT8m8kJVrJNTFuO0tjwMVo2UrXPVJ1Kv6f8ErkSAbSddm98zGmERznn1WuFvVIYYK1wqrnzCry5RI/Xr1bY6/gA1gp7pTDQWyvslcIAa4VJKQyQVtgrhQHWCnulMMBa4SIpVeK1wl4pXAbGx8bwpec+v/34wYcs7+u9ixfpuWXMcwR65zrmlOeYM1PT4/jan35x+7HXfKbqbK/59IpPgDWfXvEJsObTKz4B1nx6xSfAms+0T/Gaz16KT4A1n6k23fd/XvEJsObTKz4BzvH3ik+ANZ+k+Nz8x/bdTsl+f9vtBqyu2ue5vmB9xdIyH5unTts5dyIpOddatteNj3N+6OwhG6fUalxy7oHTdv4fHWc9+HsXrF3D1UQXfML6ieljfP5pNPgcU3Rs3PLgEw/Tsu6btp/bGxwr9WFra6fC/dKsL1tZ43i4dcOOo9Dlz7S66vTvw7ysF7ryKoQQQgghSoMGr0IIIYQQojQMNG2gUikw7FIA6mM2/V+pJpYHZ1UqKnzpemTIlWs4xNMcxQl73X1TbLxote1SeX2K1zc/b9MBf3iSp2OrxZnt++kUXnO8SY9XndWqWudL976UTje55D5/y6YKKjWefhgbtamJIvC2mHCX6osq786VVTNerK1xO3OmUqlgbNLiZGLKlZJKpry9mS01fngzm7eyAWxm81Y2gM1s3soGsJnNW9kANrN5KxvQ28zmrWwAm9m8lQ1gM5u3sgFsZvNWNoDNbLeS8lvezOatbGWgqAZMH7YD69Dxk9v3H32Sy8OUcaoY6D1dnNVUccYEbKBatf7Qm5IqSbqRNyXVkvOSNyV5SxLApiRvSQLYlOQtSQCbkrwlCWBTUi9LEsCmpJhYm7wpyVuSAO43vSUJYFOStyQBbEryliSATUkjk7w+4BpypigKTIzbNmku2rlzbJRjvtO22Jlvcrpf3Rnu0jJ63YodV6tJn3v0uJ37Rke5jz9+3O2rDh+ba13r32ZnuZ9qLXL6T71m+6QYTZZ9ZH3FyNUkvbJr55UO+JxWKSz+/NgOAIbdubhW57FPJ9oxFQOX2OqFrrwKIYQQQojSoMGrEEIIIYQoDRq8CiGEEEKI0jDQnNeIiHa0nIblpuXJrEfOJSlcveF24NyO0cOWZxKmOH/iqitrUdngRJPhulOONjl/KKxbeaIWOAekUrH8nU6byyqtrHKuT8M9Hm9wWR2fSxaq/D4dr3JtcgmaqQkrL+bz9gBgfsFKMHUi57VWCtvWa63y1CupVAqMOO3ebkphgLXCXikMsFbYK4UB1gp7pTDAWmGvFAZYK+yVwgBrhb1SGOitFfZKYYC1wl4pDLBW2CuFAdYKs1IY8N9TvVIYYK2wVwqXgXZnDVeWrCTWRN3iZiYpa1XGPEeAcx1zznPMmfV2Gx/O+fKEdt7wik+ANZ9e8Qmw5tMrPgHWfHrFJ8CaT6/4BFjz6RWfAGs+eyk+AdZ8esUnwJpPr/gEWPPpFZ8Aaz694hNgzadXfAKs+ZyZOQ7mAnJmY6NN59UQZrbvT03wuaLZtO1cS35zEqo2pllJShAuX7FYSctYeQNt7HJ/U7jSfN2kH6+4MVO3yX1BteBx0krT9uXyOvdbYcqOhzDG+3XlhvVF7SQfvuN+Z7GWjIv8uO/y3GVadvW69T9H7uO+txe68iqEEEIIIUqDBq9CCCGEEKI0DDRtoBs30GzbJeqamzqvdJNL7h0rEdEe4svjrZpdkv5olUsrhGDvMzrEU4MzQzbl3Gry5fDmhl3yXlzgqdOlW3bJfWGeX7e2zlP1Zx6yUj0LS3zp3KcNdCNP+fip8bOvv03LJsfsPadnuXRG+5abtkg+b6Wwy/pDBU9b50xRFJicsGncmUPOqHY4mWJ3ZraQTOl6M5u3sgFsZvNWNoDNbN7KBrCZzVvZADazeSsb0NvM5q1sAJvZvJUNYDObt7IBbGbzVjaAzWzeygawmc1b2cpAe6ONazdtiq/tplkbjbTsV/mmigGeLs55qhjIN+Vkvd3Fpcs7m5K8JQlgU5K3JAFsSvKWJIBNSd6SBLApyVuSADYleUsSwKakXpYkgE1J3pIEsCnJW5IANiV5SxLApiRvSQLYlLSanIe9KalSDHSY8alpt9u4ctn62Zora7XwHp/vl1ZsCvyJpz5Lyxav2nR4JfA2oJKbSWrA+xfsPYeHeH9Mz9q0+uQMX3ucmnbn/2RcUk/MlIvu/NNMUijjqn32Vo1TL9uuT+m2k36jsHW2q0u0rNm2bfHuB9wXLi/a9p2+X4YtIYQQQghxF6LBqxBCCCGEKA0avAohhBBCiNIw0GSUjU4b8y5nb+i05Vo0r3Nu0dGq5SF1EjviWt1y2eZvXqJlxbzlCHXWeWzu80qbK1xKouHKdrXb/LqNdcvDmLvMuRwPP8z5alVYblmrxZ9ptW3t7iIteWXtqVY55+3KVSvV88Elfp3Pjzx5gss6rbftM1Uq5SlrE0KBWt1ya3ZTCgOsFfZKYYC1wl4pDLBW2CuFAdYKe6UwwFphrxQGWCvslcJAb61wqgf0WuHbtYL2Pl4pDLBW2CuFAdYKTyR5bV4r7JXCZaBSqVHO7uqq7dfrC3z8lTHPEeBcx5zzHIF8FdQBFRTBtrnXfHrFJ8CaT6/4BFLFN29vfxx7xSfAmk+v+ARY8+kVnwBrPnspPgHWfHrFJ8CaT6/4BFjz6RWfAPe3XvEJsObTKz4B1nyubvA5M3fWWh1ceNuV8Vy3PrGalNGbnrHtemWOf2dQVCyXtQJ+3Yjbd/Uhzr+vDtu2e+v8m7TsRMvWV73B+7FWs/0xPprEX3LebK1aadBiKNW12v4aq9/PyyruAHDvAQC3Nuzzh6M83phvWD+y3OD1rUUbb53+/GO07Fe/YXW2R1dehRBCCCFEadDgVQghhBBClIaBpg10usDSsl0yPjpkY+e1Ck/VdZyRYW2FLzMvXbIpiu4Uv26osMvxY0M8PbOyZqW5Vld5+m9x3qaRxiZ5TH/ElWo6dfJBWjYxzpfnRwp7XEm+GzSbbnqmxlMFKys2VXXm9BladrFrl9zPvcHpDuMTriRJ5GXLDXvPWKLvKevtNlk4Jo/YlJu3sgFsZvNWNoDNbN7KBrCZ7WoyvezNbN7KBrCZzVvZADazeSsb0NvM1khsJN7M5surAWxm6yQ2LG9m81Y2gM1s3h4DsJnNW9nKQFEpMDlhFqjOhivltMjT2GWcKgZ4ujjnqeLcCc4O5k1J3pIEsCnJW5IANiV5SxKQlLJKKs55U5K3JAFsSqokfZg3JfWyJAFsSvKWJIBNSd6SBLApyVuSADYleUsSwKYkb0kC2JQUE2Nk7nQ6AUvz1u82lm17PfYkW85On7ZxxOUr79OyiQkzc8Wk/x8dc2lxyTF9+jMWA7OzfEy3WtanLSRGvUVX0rMym5jx2hzHFZdSt7jC6W/rHYvHhUU+xidXrE8bjnxuarkx3PAQL/PjvuYKL5s8aX1x/Qi3sxflGdEIIYQQQoh7Hg1ehRBCCCFEadDgVQghhBBClIYQY9z7Wfu1shA+AnARwGEAN/Z4+qC4V9tyOsZ4ZEDr+lgoTvpiUO3JNk4AxUofqE9BtnEC5NUe9SnINlbu1bbsGisDHbxurzSEV2OMzw58xTugtuRLTtsjp7YA+bXnoMlpe6gt+ZLb9sipPTm1JQdy2h5qy+0obUAIIYQQQpQGDV6FEEIIIURpOKjB6w8OaL07obbkS07bI6e2APm156DJaXuoLfmS2/bIqT05tSUHctoeakvCgeS8CiGEEEII8UlQ2oAQQgghhCgNAx28hhC+GkJ4K4RwPoTwwiDXvbX+fwkhXA8hvOH+NxtC+HkI4Z2tvzO93mMf23IqhPCLEMK5EMLZEMK3D7I9uXGQsaI4KQ/qU6gtipUeqE/ZXq/ipAfqU6gt2cbKwAavIYQCwD8D+BqAxwH8TQjh8UGtf4sXAXw1+d8LAF6OMT4C4OWtx4NgA8B3Y4yPAXgOwLe2tsdBtScbMoiVF6E4yZ4M4gRQrJSCDGLlRShOsieDOAEUK/0RYxzIDcAXAfyne/w9AN8b1Prdes8AeMM9fgvAia37JwC8Neg2ba37ZwC+kkt7DvKWQ6woTvK/5RAnipVy3HKIFcVJ/rcc4kSx0t9tkGkDJwF86B5f2vrfQXMsxjgHAFt/jw66ASGEMwCeAfDrHNqTATnGyoHvF8XJbeQYJ0AG+0axchs5xsqB7xfFyW3kGCdABvsmt1gZ5OA17PC/e77UQQhhHMBPAHwnxrh00O3JBMVKguJkRxQnO6BY2RHFSoLiZEcUJzuQY6wMcvB6CcAp9/h+AFcGuP7duBZCOAEAW3+vD2rFIYQaNgPixzHGnx50ezIix1hRnORHjnECKFZyJMdYUZzkR45xAihWbmOQg9dXADwSQngghDAE4K8BvDTA9e/GSwCe37r/PDZzOu44IYQA4IcAzsUYv3/Q7cmMHGNFcZIfOcYJoFjJkRxjRXGSHznGCaBYuZ0BJ/t+HcDbAC4A+McDSDb+VwBzANrY/Ib1TQCHsPlruXe2/s4OqC1/jM3piNcA/G7r9vWDak9ut4OMFcVJeW7qUxQrZYgVxUl5bupTyhErMmwJIYQQQojSIMOWEEIIIYQoDRq8CiGEEEKI0qDBqxBCCCGEKA0avAohhBBCiNKgwasQQgghhCgNGrwKIYQQQojSoMGrEEIIIYQoDRq8CiGEEEKI0vB/eXn7FWjDd5EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#riscaliamo i valori tra 0 e 1 per la visualizzazione\n",
    "def norm(im):\n",
    "    im = im-im.min()\n",
    "    return im/im.max()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(norm(cifar100_train[0][0].numpy().transpose(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo notare, ogni volta che lo stesso campione viene caricato, vengono applicate ad esso trasformazioni diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 11**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Quali sono i vantaggi del processo di data augmentation appena descritto rispetto ad applicare le trasformazioni considerate a tutte le immagini di training per ottenere una versione aumentata del dataset da conservare in memoria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 11**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adattiamo adesso MiniAlexNet per lavorare su immagini $28 \\times 28$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MiniAlexNetV3(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNetV3, self).__init__() \n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature maps\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            #Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), #Input: 3 x 28 x 28. Ouput: 16 x 28 x 28\n",
    "            nn.MaxPool2d(2), #Input: 16 x 28 x 28. Output: 16 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv2\n",
    "            nn.Conv2d(16, 32, 5, padding=2), #Input 16 x 14 x 14. Output: 32 x 14 x 14\n",
    "            nn.MaxPool2d(2), #Input: 32 x 14 x 14. Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv3\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #Input 32 x 7 x 7. Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv4\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #Input 64 x 7 x 7. Output: 128 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv5\n",
    "            nn.Conv2d(128, 256, 3, padding=1), #Input 128 x 7 x 7. Output: 256 x 7 x 7\n",
    "            nn.MaxPool2d(2), #Input: 256 x 7 x 7. Output: 256 x 3 x 3\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(), #i layer di dropout vanno posizionati prima di FC6 e FC7\n",
    "            #FC6\n",
    "            nn.Linear(2304, 2048), #Input: 256 * 3 * 3\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            #FC7\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC8\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 12**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Cosa è stato modificato nel modello? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 12**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_v3_cifar100 = MiniAlexNetV3()\n",
    "mini_alexnet_v3_cifar100 = train_classifier(mini_alexnet_v3_cifar100, cifar100_train_loader, cifar100_test_loader, \\\n",
    "                                  'minialexnet_v3_cifar100', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_v3_cifar100_tb2.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test e confrontiamo con gli altri modelli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su CIFAR-100: 0.24\n",
      "Accuracy MiniAlexNet su CIFAR-100: 0.21\n",
      "Accuracy MiniAlexNetV2 su CIFAR-100: 0.40\n",
      "Accuracy MiniAlexNetV3 su CIFAR-100: 0.49\n"
     ]
    }
   ],
   "source": [
    "minialexnet_v3_cifar100_test_predictions, cifar100_labels_test = test_classifier(mini_alexnet_v3_cifar100, cifar100_test_loader)\n",
    "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,lenet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNet su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV2 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v2_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV3 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v3_cifar100_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Batch Normalization\n",
    "Un'altra tecnica che permette di regolarizzare il modello è quella della batch normalization. La batch normalization permette di ridurre la varianza delle attivazioni dei layer intermedi della rete e può essere applicata inserendo nell'architettura il layer `nn.BatchNorm2d` nel caso di layer di convoluzioni e `nn.BatchNorm1d` nel caso di layer fully connected. I liveli di batch normalization vanno inseriti prima di ogni layer eccetto il primo. Modifichiamo AlexNetV4 introducendo la batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MiniAlexNetV4(nn.Module):\n",
    "    def __init__(self, input_channels=3, out_classes=100):\n",
    "        super(MiniAlexNetV4, self).__init__() \n",
    "        #ridefiniamo il modello utilizzando i moduli sequential.\n",
    "        #ne definiamo due: un \"feature extractor\", che estrae le feature maps\n",
    "        #e un \"classificatore\" che implementa i livelly FC\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            #Conv1\n",
    "            nn.Conv2d(input_channels, 16, 5, padding=2), #Input: 3 x 28 x 28. Ouput: 16 x 28 x 28\n",
    "            nn.MaxPool2d(2), #Input: 16 x 28 x 28. Output: 16 x 14 x 14\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv2\n",
    "            nn.BatchNorm2d(16), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Conv2d(16, 32, 5, padding=2), #Input 16 x 14 x 14. Output: 32 x 14 x 14\n",
    "            nn.MaxPool2d(2), #Input: 32 x 14 x 14. Output: 32 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv3\n",
    "            nn.BatchNorm2d(32), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #Input 32 x 7 x 7. Output: 64 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv4\n",
    "            nn.BatchNorm2d(64), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #Input 64 x 7 x 7. Output: 128 x 7 x 7\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #Conv5\n",
    "            nn.BatchNorm2d(128), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Conv2d(128, 256, 3, padding=1), #Input 128 x 7 x 7. Output: 256 x 7 x 7\n",
    "            nn.MaxPool2d(2), #Input: 256 x 7 x 7. Output: 256 x 3 x 3\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(), #i layer di dropout vanno posizionati prima di FC6 e FC7\n",
    "            #FC6\n",
    "            nn.BatchNorm1d(2304), #dobbiamo passare come parametro il numero di feature in input\n",
    "            nn.Linear(2304, 2048), #Input: 256 * 3 * 3\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            #FC7\n",
    "            nn.BatchNorm1d(2048), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            #FC8\n",
    "            nn.BatchNorm1d(1024), #dobbiamo passare come parametro il numero di mappe di feature in input\n",
    "            nn.Linear(1024, out_classes)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Applichiamo le diverse trasformazioni in cascata\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x.view(x.shape[0],-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 13**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti il codice scritto sopra con quello di AlexNetv3, cosa cambia? Perché non viene applicata la batch normalizzation al primo layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 13**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_v4_cifar100 = MiniAlexNetV4()\n",
    "mini_alexnet_v4_cifar100 = train_classifier(mini_alexnet_v4_cifar100, cifar100_train_loader, cifar100_test_loader, \\\n",
    "                                  'minialexnet_v4_cifar100', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_v4_cifar100_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confrontiamo l'accuracy di test con quella degli altri modelli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LeNetColor su CIFAR-100: 0.24\n",
      "Accuracy MiniAlexNet su CIFAR-100: 0.21\n",
      "Accuracy MiniAlexNetV2 su CIFAR-100: 0.40\n",
      "Accuracy MiniAlexNetV3 su CIFAR-100: 0.49\n",
      "Accuracy MiniAlexNetV4 su CIFAR-100: 0.59\n"
     ]
    }
   ],
   "source": [
    "minialexnet_v4_cifar100_test_predictions, cifar100_labels_test = test_classifier(mini_alexnet_v4_cifar100, cifar100_test_loader)\n",
    "print(\"Accuracy LeNetColor su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,lenet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNet su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV2 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v2_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV3 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v3_cifar100_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV4 su CIFAR-100: %0.2f\" % \\\n",
    "    accuracy_score(cifar100_labels_test,minialexnet_v4_cifar100_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 14**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confrontino le curve di test di MiniAlexNetV3 con quelle di MiniAlexNetV4. Ci sono delle differenze significative? Quali?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 14**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Transfer Learning\n",
    "\n",
    "Uno dei vantaggi delle reti neurali è che, una volta allenate per risolvere un dato problema, esse possono essere riutilizzate per risolvere nuovi task mediante la tecnica del \"fine-tuning\". Ciò consiste semplicemente nell'inizializzare i parametri del modello con quelli provenienti da un modello allenato per un task diverso. Consideriamo ad esempio il dataset CIFAR-10. Esso è un sottoinsieme di CIFAR-100, che contiene etichette per 10 classi invece di 100. Compareremo le performance di un modello MiniAlexNet allenato da zero con il modello MiniAlexNet pre-allenato su CIFAR-100. Carichiamo il dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([transforms.RandomVerticalFlip(),\n",
    "                                      transforms.ColorJitter(),\n",
    "                                      transforms.RandomCrop(28),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "#in fase di test specifichiamo solo il crop centrale (non potremmo classificare immagini 32 x 32)\n",
    "transform_test = transforms.Compose([ transforms.CenterCrop(28),\n",
    "                                      transforms.ToTensor(), \n",
    "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "cifar10_train = CIFAR10(root='cifar10',train=True, download=True, transform=transform_train)\n",
    "cifar10_test = CIFAR10(root='cifar10',train=False, download=True, transform=transform_test)\n",
    "cifar10_train_loader = DataLoader(cifar10_train, batch_size=1024, num_workers=2, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test, batch_size=1024, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo il modello da zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_v4_cifar10 = MiniAlexNetV4()\n",
    "mini_alexnet_v4_cifar10 = train_classifier(mini_alexnet_v4_cifar10, cifar10_train_loader, cifar10_test_loader, \\\n",
    "                                  'minialexnet_v4_cifar10', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_v4_cifar10_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MiniAlexNetV4 su CIFAR-10: 0.81\n"
     ]
    }
   ],
   "source": [
    "minialexnet_v4_cifar10_test_predictions, cifar10_labels_test = test_classifier(mini_alexnet_v4_cifar10, cifar10_test_loader)\n",
    "print(\"Accuracy MiniAlexNetV4 su CIFAR-10: %0.2f\" % \\\n",
    "    accuracy_score(cifar10_labels_test,minialexnet_v4_cifar10_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo adesso come adattare il modello già allenato su CIFAR-100 per CIFAR-10. Visualizziamo il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniAlexNetV4(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=2304, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=1024, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_alexnet_v4_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'input del modello è sempre costituito da immagini $32 \\times 32$. L'output però è pari a $100$ classi per il modello di partenza, mentre CIFAR-10 contiene $10$ classi. Dobbiamo adattare il modello per il nuovo task. Per farlo, conserveremo tutti i layer tranne l'ultimo, che verrà distrutto e ricreato. Facciamo una copia del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "mini_alexnet_v4_cifar100_cifar10 = deepcopy(mini_alexnet_v4_cifar100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accediamo al modulo sequential \"classifier\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Linear(in_features=2304, out_features=2048, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Dropout(p=0.5, inplace=False)\n",
       "  (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Linear(in_features=1024, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = mini_alexnet_v4_cifar100_cifar10.classifier\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraiamo i moduli contenuti in \"classifier\" in una lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2304, out_features=2048, bias=True),\n",
       " ReLU(),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2048, out_features=1024, bias=True),\n",
       " ReLU(),\n",
       " BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=1024, out_features=100, bias=True)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_mods = list(classifier)\n",
    "classifier_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rimuoviamo l'ultimo elemento della lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2304, out_features=2048, bias=True),\n",
       " ReLU(),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2048, out_features=1024, bias=True),\n",
       " ReLU(),\n",
       " BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_mods.pop()\n",
    "classifier_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inseriamo un nuovo layer lineare per ottenere in output $10$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2304, out_features=2048, bias=True),\n",
       " ReLU(),\n",
       " Dropout(p=0.5, inplace=False),\n",
       " BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=2048, out_features=1024, bias=True),\n",
       " ReLU(),\n",
       " BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=1024, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_mods.append(nn.Linear(1024, 10))\n",
    "classifier_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso possiamo trasformare classifier_mods in un oggetto di tipo Sequential e inserirlo dentro il modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniAlexNetV4(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=2304, out_features=2048, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_alexnet_v4_cifar100_cifar10.classifier=nn.Sequential(*classifier_mods)\n",
    "mini_alexnet_v4_cifar100_cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Va notato che questa operazione ha cambiato solamente l'ultimo livello, lasciando gli altri invariati. Tutti i livelli tranne il l'ultimo contengono dunque i pesi \"pre-allenati\" su CIFAR-100. Procediamo all'allenamento del modello:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_alexnet_v4_cifar100_cifar10 = train_classifier(mini_alexnet_v4_cifar100_cifar10, cifar10_train_loader, \\\n",
    "                                                    cifar10_test_loader, 'minialexnet_v4_cifar100_cifar10', epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le curve di training/test dovrebbero essere simili alle seguenti:\n",
    "\n",
    "<center><img src='img/mini_alexnet_v4_cifar100_cifar10_tb.jpg' width=80%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 15**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti la curva di training relativa al modello fine-tuned con quella relativa al modello allenato da zero. Quale dei due modelli converge prima? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 15**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo l'accuracy di test e confrontiamola con quella del modello allenato da zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MiniAlexNetV4 su CIFAR-10: 0.81\n",
      "Accuracy MiniAlexNetV4 pre-allenato su CIFAR-100 e fine-tuned su CIFAR-10: 0.83\n"
     ]
    }
   ],
   "source": [
    "minialexnet_v4_cifar100_cifar10_test_predictions, cifar10_labels_test = test_classifier(mini_alexnet_v4_cifar100_cifar10,\\\n",
    "                                                                                        cifar10_test_loader)\n",
    "print(\"Accuracy MiniAlexNetV4 su CIFAR-10: %0.2f\" % \\\n",
    "    accuracy_score(cifar10_labels_test,minialexnet_v4_cifar10_test_predictions))\n",
    "print(\"Accuracy MiniAlexNetV4 pre-allenato su CIFAR-100 e fine-tuned su CIFAR-10: %0.2f\" % \\\n",
    "    accuracy_score(cifar10_labels_test,minialexnet_v4_cifar100_cifar10_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 16**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "I due modelli in questo caso hanno raggiunto accuracy simili. Qual è stato il vantaggio del fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 16**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch mette a disposizione diversi modelli pre-allenati che possono essere utilizzati come punto di partenza per risolvere nuovi task. Una lista dei modelli è disponibile qui: <a href=\"https://pytorch.org/docs/master/torchvision/models.html\">https://pytorch.org/docs/master/torchvision/models.html</a>. I modelli sono stati pre-allenati su ImageNet, un dataset contente milioni di immagini suddivise in 1000 categorie. Nei prossimi laboratori vedremo più in dettaglio come effettuare fine-tuning a partire da questi modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizi\n",
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 1**\n",
    "\n",
    "I dataset visti in questo laboratorio sono piuttosto \"semplici\", per cui risulta difficile apprezzare le differenze che si hanno con l'introduzione delle diverse tecniche di regolarizzazione. Si alleni un modello simile a MiniAlexNet per classificare il dataset 8Scenes visto negli scorsi laboratori. Il modello deve prendere in input immagini di dimensioni $224 \\times 224$, estratte dalla parte centrale delle immagini del dataset (si utilizzi la trasformazione \"CenterCrop\"). Si considerino diverse versioni del modello con e senza dropout e con e senza batch normalization. Quale dei due modelli ottiene risultati migliori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 2**\n",
    "\n",
    "Si estenda l'esercizio 1 introducendo le tecniche di data augmentation viste in questo laboratorio. I risultati migliorano?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 3**\n",
    "\n",
    "Si carichi il modello AlexNet specificando il flag \"pretrained=True\" (vedere documentazione - https://pytorch.org/docs/master/torchvision/models.html). Si modifichi il modello per risolvere il task di classificazione sul dataset 8Scenes. Si esegua il fine-tuning del modello. Il modello ottiene risultati migliori rispetto al modello allenato da zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    " * Documentazione di PyTorch. http://pytorch.org/docs/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
