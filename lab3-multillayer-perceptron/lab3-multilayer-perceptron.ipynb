{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".rendered_html * + p, .rendered_html p {\n",
    "    text-align:justify;\n",
    "}\n",
    ".print {\n",
    "    display:none;\n",
    "}\n",
    ".highlight {\n",
    "    background:white;\n",
    "}\n",
    "@media print {\n",
    " a[href]:after {\n",
    "     content: \"\"\n",
    " }\n",
    " .noprint {\n",
    "  display:none\n",
    "  }\n",
    "  .print {\n",
    "        display:block;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "<head>\n",
    "    <base target=\"_blank\">\n",
    "</head>\n",
    "<div style=\"text-align:left\"><a href=\"http://web.dmi.unict.it/\"><img src=\"img/dmi.png\" style=\"width:300px; margin:0;\"></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://iplab.dmi.unict.it/\"><img src=\"img/iplab.png\" style=\"width:900px\"></a>\n",
    "<center><h2>Machine Learning - A.A. 2020-2021</h2></center>\n",
    "<center><h3>Softmax, Stochastic Gradient Descent, MLP, Caricamento Dati con PyTorch</h3></center>\n",
    "<br>\n",
    "<center>Antonino Furnari - <a href=\"http://www.dmi.unict.it/~furnari/\" target=\"_blank\">http://www.dmi.unict.it/~furnari/</a> - <a href=\"mailto:furnari@dmi.unict.it\">furnari@dmi.unict.it</a> </center>\n",
    "<center>Giovanni Maria Farinella - <a href=\"http://www.dmi.unict.it/farinella/\" target=\"_blank\">http://www.dmi.unict.it/farinella/</a> - <a href=\"mailto:gfarinella@dmi.unict.it\">gfarinella@dmi.unict.it</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classificazione SoftMax\n",
    "\n",
    "Abbiamo visto come sia possibile \"trasformare\" un regressore lineare in un classificatore binario utilizzando la funzione logistica. Abbiamo anche visto che è possibile implementare un classificatore multiclasse mediante il principio \"one-vs-all\". Tuttavia, il principio one-vs-all è poco naturale per risolvere problemi di classificazione multiclasse. Un regressore logistico ci permette di stimare la probabilità:\n",
    "\n",
    "\\begin{equation}\n",
    "p(c\\ |\\ x)\n",
    "\\end{equation}\n",
    "\n",
    "dove $c$ è la classe ($c=0$ nel caso della classe negativa e $c=1$ nel caso della classe positiva) e $x$ è il campione in ingresso. Sappiamo inoltre che\n",
    "\n",
    "\\begin{equation}\n",
    "p(c=0\\ |\\ x)+p(c=1\\ |\\ x)=1\n",
    "\\end{equation}\n",
    "\n",
    "per cui il un regressore logistico ci permette di stimare la distribuzione di probabilità condizionale sulle classi possibili (solo due in questo caso), dato il campione in ingresso $x$.\n",
    "\n",
    "Supponiamo adesso di avere un problema di classificazione su $K$ classi $c=0, c=1, \\ldots, c=K-1$. Il principio \"one-vs-all\" ci permette di classificare gli elementi $x$, ma non di stimare direttamente una distribuzione di probabilità condizionale sulle tre classi dato il campione in ingresso:\n",
    "\n",
    "\\begin{equation}\n",
    "p(c\\ |\\ x)\\ :\\ p(c=0\\ |\\ x)+p(c=1\\ |\\ x)+\\ldots + p(c=K\\ |x\\ )=1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 1**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "A cosa può servire stimare le probabilità a posteriori $p(c\\ |\\ x)$ oltre a inferire la classe più probabile per il campione $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 1**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se volessimo ottenere una distribuzione di probabilità sulle $K$ classi, potremmo pensare di costruire un regressore lineare che, preso in input un dato, restituisce un vettore di $K$ elementi. Ciò può essere ottenuto semplicemente con una trasformazione lineare del tipo $\\mathbf{z}=A\\mathbf{x} + \\mathbf{b}$, dove $A$ è una matrice $n\\times K$, con $n$ numero di feature in ingresso. Analogamente a quanto visto nel caso del regressore logistico, tuttavia, non vi è alcuna garanzia che il vettore $\\mathbf{z}$ rappresenti una valida distribuzione di probabilità. Ricordiamo che affinché ciò accada ci serve che:\n",
    " * $z_i\\geq0,\\ \\forall i \\in \\{1,\\ldots,K\\}$;\n",
    " * $\\sum_{i=1}^K z_i = 1$.\n",
    "\n",
    "La funzione SoftMax è una generalizzazione della funzione logistica che ci permette di normalizzare un vettore arbitrario di numeri in modo che rispetti le due proprietà appena viste:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(\\mathbf{x})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}}\n",
    "\\end{equation}\n",
    "\n",
    "dove $z_j$ rappresenta la j-esima componente del vettore $\\mathbf{z}$ (e dunque $\\sigma(\\mathbf{x})_j$ rappresenta la j-esima componente del vettore normalizzato mediante SoftMax $\\sigma(\\mathbf{x})$). In pratica, la funzione SoftMax esegue due operazioni:\n",
    " * Applica la funzione esponenziale a tutte le componenti del vettore non normalizzato ($e^{z_i}$). Questa operazione permette di soddisfare la prima proprietà mappando numeri $x \\in ]-\\infty, +\\infty[$ su numeri del range $[0,+\\infty[$. Si noti che la funzione esponenziale è monotona crescente, per cui se $z_i \\leq z_j$, allora $e^{z_i} \\leq e^{z_j}$;\n",
    " * Normalizza gli elementi del vettore in uscita dividendoli per la somma dei valori positivi $e^{z_i}$ ($\\frac{e^{z_i}}{\\sum z_k}$). Questa normalizzazione ci assicura che la seconda proprietà sia rispettata: $\\sum_{i=1}^K \\sigma(\\mathbf{z})_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 2**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Cosa garantisce che i valori restituiti dalla funzione SoftMax siano non negativi? Disegnare la funzione esponenziale per rispondere alla domanda. E' possibile ottenere delle probabilità nulle utilizzando la funzione SoftMax in teoria? E' possibile in pratica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 2**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formulazione del regressore SoftMax è dunque la seguente:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}) = \\sigma(A \\mathbf{x} +\\mathbf{b}) = \\sigma(\\mathbf{z})= \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}}\n",
    "\\end{equation}\n",
    "\n",
    "dove $\\mathbf{z}=A \\mathbf{x} +\\mathbf{b}$, la funzione f stima la probabilità a posteriori che $\\mathbf{x}$ appartenga ad una data classe:\n",
    "\n",
    "\\begin{equation}\n",
    "p(c=i\\ |\\ \\mathbf{x}) = f(\\mathbf{x})_i\n",
    "\\end{equation}\n",
    "\n",
    "e $f(\\mathbf{x})_i$ indica la iesima componente del vettore di probabilità ottenuto mediante il regressore softmax $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per allenare il regressore softmax, utilizziamo una generalizzazione della loss vista nel caso del regressore logistico: la **cross entropy loss**. In teoria dell'informazione, la cross entropy tra due distribuzioni di probabilità $p$ e $q$ è definita come:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p,q) = -\\sum_x p(x) \\log q(x)\n",
    "\\end{equation}\n",
    "\n",
    "La cross entropy $H(p,q)$ indica il *numero medio di bit necessario per identificare eventi $x$ che seguono la probabilità $p$ se li descriviamo con la probabilità stimata $q$*. La cross entropy raggiunge il suo minimo quando $p$ e $q$ sono uguali. In tal caso la cross entropy corrisponde all'entropia di $p$:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p) = -\\sum_x p(x) \\log p(x)\n",
    "\\end{equation}\n",
    "\n",
    "Nel nostro caso, la probabilità $q$ è data dal regressore softmax, mentre $p$ rappresenta la probabilità \"ideale\" che il campione $x$ appartenga a una data classe. Dato che conosciamo le classi di appartenenza di ogni campione, la probabilità ideale è data da una rappresentazione di tipo \"one-hot-vector\", in cui $p(\\mathbf{x})=\\mathbf{y}$ e $\\mathbf{y}$ ha una unica componente $y_j=1$, mentre tutte le altre sono nulle. Ad esempio, se le classi sono tre e il campione appartiene alla seconda classe ($c=1$), allora $\\mathbf{y}=[0,1,0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 3**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "I vettori \"one-hot\" $\\mathbf{y}$ sono delle valide distribuzioni di probabilità?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 3**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo dunque scrivere la loss relativa a un dato campione $\\mathbf{x}$ di etichetta (one-hot) $\\mathbf{y}$ come segue:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_\\theta(\\mathbf{x},\\mathbf{y}) = -\\sum_i \\mathbf{y}_i \\log f(\\mathbf{x})_i\n",
    "\\end{equation}\n",
    "\n",
    "Notiamo che, $\\mathbf{y}_i$ sarà uguale a zero tranne che per $i=j$, dove $j$ è la classe del campione $\\mathbf{x}$. Pertanto, solo uno dei termini della sommatoria nella formula sopra sarà non nullo. Ciò ci permette di riscrivere la loss come segue:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_\\theta(\\mathbf{x},j) = - \\log f(\\mathbf{x})_j\n",
    "\\end{equation}\n",
    "\n",
    "Dove $j$ è la classe di $\\mathbf{x}$. Ricordando che $f(\\mathbf{x}) = \\sigma(A \\mathbf{x} +\\mathbf{b}) = \\sigma(\\mathbf{z})= \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}}$, possiamo riscrivere la loss come:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_\\theta(\\mathbf{x},j) = - \\log \\frac{e^{z_j}}{\\sum_{k=1}^K{e^{z_k}}} = \\log \\sum_{k=1}^K{e^{z_k}} - \\log {e^{z_j}}\n",
    "\\end{equation}\n",
    "\n",
    "da cui, ricordando che $\\mathbf{z}=A \\mathbf{x} +\\mathbf{b}$, abbiamo:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_\\theta(\\mathbf{x},j) = \\log \\sum_{k=1}^K{e^{z_k}} -z_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 4**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Le loss $\\mathcal{L}_\\theta(\\mathbf{x},j) = - \\log f(\\mathbf{x})_j$ e $\\mathcal{L}_\\theta(\\mathbf{x},j) = \\log \\sum_{k=1}^K{e^{z_k}} -z_j$ sono entrambe valide? Una delle due offre dei vantaggi rispetto all'altra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 4**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Implementazione di un Regressore Softmax\n",
    "Implementiamo un regressore softmax. Iniziamo caricando il dataset delle iris di Fisher. Si tratta di un dataset contenente le misurazioni di $4$ quantità relative a $150$ fiori appartenenti a $3$ specie diverse. Il dataset viene spesso utilizzato per illustrare il funzionamento degli algoritmi di classificazione multiclasse, considerando il problema di stimare la specie di appartenenza di ciascun fiore a partire dalle misurazioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X=iris.data\n",
    "Y=iris.target\n",
    "#features\n",
    "print(X.shape)\n",
    "#classi target\n",
    "print(Y.shape)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati dunque caratterizzati da:\n",
    " * $4$ features;\n",
    " * $3$ classi;\n",
    " * $150$ istanze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impostiamo un seed per avere risultati ripetibili:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(1234);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otteniamo una permutazione casuale dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applichiamo la stessa permutazione a X e Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[idx]\n",
    "Y = Y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddividiamo il dataset in **training** e **testing** set indipendenti selezionando i primi $30$ valori per formare il testing set. Trasformiamo inoltre gli array in tensori:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "X_training = Tensor(X[30:])\n",
    "Y_training = Tensor(Y[30:])\n",
    "X_testing = Tensor(X[:30])\n",
    "Y_testing = Tensor(Y[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizziamo i dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = X_training.mean(0)\n",
    "X_std = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training-X_mean)/X_std\n",
    "X_testing_norm = (X_testing-X_mean)/X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo dunque un nuovo modulo per effettuare la regressione softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SoftMaxRegressor(nn.Module):\n",
    "    def __init__(self, in_features, out_classes):\n",
    "        \"\"\"Costruisce un regressore softmax.\n",
    "            Input:\n",
    "                in_features: numero di feature in input (es. 4)\n",
    "                out_classes: numero di classi in uscita (es. 3)\"\"\"\n",
    "        super(SoftMaxRegressor, self).__init__() #richiamo il costruttore della superclasse\n",
    "        #questo passo è necessario per abilitare alcuni meccanismi automatici dei moduli di PyTorch\n",
    "        \n",
    "        self.linear = nn.Linear(in_features,out_classes) #il regressore softmax restituisce \n",
    "        #distribuzioni di probabilità, quindi il numero di feature di output coincide con il numero di classi\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"Definisce come processare l'input x\"\"\"\n",
    "        scores = self.linear(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiamo un regressore softmax e passiamogli i dati di training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9326,  0.6582, -0.2940],\n",
       "        [-0.1301, -0.2885,  0.1746],\n",
       "        [ 1.1440,  1.6170, -1.0236],\n",
       "        [-0.1766, -0.4061,  0.2196],\n",
       "        [-0.4706, -0.5166,  0.2307],\n",
       "        [ 1.2162,  1.5326, -0.9408],\n",
       "        [ 1.6219,  1.6549, -0.9210],\n",
       "        [ 0.8984,  1.2598, -0.8457],\n",
       "        [ 1.4105,  1.9569, -1.2018],\n",
       "        [ 1.0097,  0.5520, -0.2012]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoftMaxRegressor(4,3)# 4 feature in ingresso, 3 classi in uscita\n",
    "#mostriamo le prime 4 predizioni\n",
    "model(X_training_norm)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni riga della matrice è una predizione. Come si può notare, non si tratta di valide distribuzioni di probabilità. Per ottenere le distribuzioni dobbiamo utilizzare la funzione softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4870, 0.3701, 0.1428],\n",
       "        [0.3116, 0.2659, 0.4225],\n",
       "        [0.3678, 0.5902, 0.0421],\n",
       "        [0.3048, 0.2423, 0.4529],\n",
       "        [0.2518, 0.2405, 0.5077],\n",
       "        [0.4020, 0.5516, 0.0465],\n",
       "        [0.4734, 0.4893, 0.0372],\n",
       "        [0.3831, 0.5499, 0.0670],\n",
       "        [0.3571, 0.6167, 0.0262],\n",
       "        [0.5180, 0.3277, 0.1543]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)#dim=1 specifica che effettueremo il sofmax per righe\n",
    "softmax(model(X_training_norm))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni riga della matrice è adesso una valida distribuzione di probabilità sulle tre classi considerate. Infatti, la somma dei valori lungo le righe è, come ci si aspetterebbe, pari ad 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(model(X_training_norm)).sum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta allenato, il modello ci permetterà di predire una distribuzione di probabilità per ogni elemento. Per ottenere l'etichetta predetta, possiamo applicare il principo Maximum A Posteriori (MAP), scegliendo la classe che presenta la probabilità maggiore mediante la funzione argmax, che è inclusa in PyTorch nella funzione max:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 2, 2, 1, 1, 1, 1, 0, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "        0, 0, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 2, 0, 2, 2, 0, 1, 2, 1, 2, 1, 0,\n",
       "        0, 1, 0, 1, 2, 2, 2, 1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 2, 2, 0, 1, 1, 0, 0,\n",
       "        1, 2, 0, 2, 2, 0, 1, 2, 2, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2, 1, 1,\n",
       "        1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 1, 2, 1, 1, 2, 2, 0, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#la funzione max restituisce i valori dei massimi \n",
    "#e i loro indici (il risultato della funzione argmax)\n",
    "#per questo includiamo \"[1]\" nell'equazione successiva\n",
    "preds = softmax(model(X_training_norm)).max(1)[1]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo dunque ottenuto le predizioni sotto forma di indici delle tre clasis, che vanno da $0$ a $2$. Possiamo dunque valutare le predizioni come visto nel caso binario. Ad esempio, possiamo calcolare l'accuracy come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35833333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_training,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy è bassa in quanto dobbiamo ancora allaenare il modello. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Va notato che, dato che la funzione softmax è monotona, possiamo applicare la funzione argmax direttamente ai logits ottenendo lo stesso risultato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "preds_logits = model(X_training_norm).max(1)[1]\n",
    "print((preds_logits==preds).float().mean()) #il risultato ottenuto è lo stesso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pratica, si preferisce dunque non applicare la funzione softmax per il calcolo delle etichette predette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La procedura di training del regressore logistico sarà la seguente:\n",
    "\n",
    "1. Normalizzare i dati in ingresso $\\mathbf{x}$; \n",
    "2. Costruire il modulo che implementa il modello (il costruttore si preoccuperà di inizializzare i parametri);\n",
    "3. Mettere il modello in modalità \"training\";\n",
    "4. Calcolare l'output del modello $\\hat y$;\n",
    "5. Calcolare il valore della loss $\\mathcal{L}_\\theta(\\mathbf{x},y)$;\n",
    "6. Calcolare il gradiente della loss rispetto ai parametri del modello;\n",
    "7. Aggiornare i pesi $\\theta$ utilizzando il gradient descent;\n",
    "8. Ripetere i passi 4-7 fino a convergenza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementiamo la procedura includendo il monitoring delle curve mediante tensorboard e il calcolo dell'accuracy ad ogni iterazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "\n",
    "writer = SummaryWriter('logs/softmax_regressor')\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 500\n",
    "\n",
    "#normalizzazione dei dati\n",
    "X_mean = X_training.mean(0)\n",
    "X_std = X_training.std(0)\n",
    "\n",
    "X_training_norm = (X_training-X_mean)/X_std\n",
    "X_testing_norm = (X_testing-X_mean)/X_std\n",
    "\n",
    "model = SoftMaxRegressor(4,3)\n",
    "criterion = nn.CrossEntropyLoss() #utilizziamo la cross entropy loss\n",
    "optimizer = SGD(model.parameters(),lr) #utilizziamo un optimizer\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    out = model(X_training_norm)\n",
    "    l = criterion(out,Y_training.long())\n",
    "    l.backward()\n",
    "    writer.add_scalar('loss/train', l.item(), global_step=e)\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    preds_train = out.max(1)[1]\n",
    "    writer.add_scalar('accuracy/train', accuracy_score(Y_training,preds_train), global_step=e)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        out = model(X_testing_norm)\n",
    "        l = criterion(out, Y_testing.long())\n",
    "        writer.add_scalar('loss/test', l.item(), global_step=e)\n",
    "        preds_test = out.max(1)[1]\n",
    "        writer.add_scalar('accuracy/test', accuracy_score(Y_testing,preds_test), global_step=e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo accuracy di training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy di training 0.9583333333333334\n",
      "Accuracy di test 1.0\n"
     ]
    }
   ],
   "source": [
    "preds_train = model(X_training_norm).max(1)[1]\n",
    "preds_test = model(X_testing_norm).max(1)[1]\n",
    "print(\"Accuracy di training\",accuracy_score(Y_training,preds_train))\n",
    "print(\"Accuracy di test\",accuracy_score(Y_testing,preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 5**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti la procedura di training appena vista con quelle viste in precedenza. Quali sono le principali differenze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 5**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datasets, Data Loaders, Stochastic Gradient Descent, Salvataggio e Caricamento dei Modelli\n",
    "\n",
    "Finora abbiamo effettuato la discesa del gradiente calcolando i gradienti rispetto alla loss calcolata sull'intero dataset. Questa procedura è nota come \"Batch Gradient Descent\". In pratica, se il dataset è molto grande, questa procedura può essere infattibile (se il dataset è grande, potrebbe essere difficile tenerlo tutto in memoria RAM). Per superare questi limiti, è possibile utilizzare la tecnica della \"Stochastic Gradient Descent\" (SGD). Questa tecnica consiste nel suddividere il dataset in una serie di mini-batch e effettuare la discesa del gradiente su un batch alla volta. Vediamo un esempio di training mediante Stochastic Gradient Descent considerando un esempio di dataset più grande.\n",
    "\n",
    "Considereremo il dataset MNIST. Questo dataset contiene $70,000$ immagini monocromatiche $28 \\times 28$ pixels raffiguranti cifre scritte a mano (da $0$ a $9$). Ogni immagine è classificata in relazione alla cifra contenuta nell'immagine. Le immagini sono suddivise come segue: $60,000$ immagini costituiscono il training set, mentre le restanti $10,000$ costisuiscono il test set.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/mnist.png\">\n",
    "</center>\n",
    "\n",
    "PyTorch mette a disposizione una serie di oggetti per caricare dataset noti (come MNIST) e suddividerli in mini-batch in modo da effettuare la stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset e Trasformazioni\n",
    "L'oggetto `MNIST` fornito da PyTorch permette di scaricare il dataset sul proprio computer e utilizzarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5d1dc364764c2aa42086d162b5d024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e47842b8d47aabec1fca7cd91e9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077dbb7ad15b443eb453d142239a3989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0596d75b314612b2557e8ab3d62202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "#root='data' indica di scaricare il dataset nella sottocartella \"data\" della cartella corrente\n",
    "#train=True indica che vogliamo caricare il training set\n",
    "#download=True indica di scaricare il dataset se non è già presente nella directory specificata\n",
    "mnist_train = MNIST(root='data',train=True, download=True)\n",
    "#test set\n",
    "mnist_test = MNIST(root='data',train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si noti che, la prima volta che si esegue il codice mostrato sopra, il dataset verrà scaricato nella cartella \"data\", mentre nelle volte successive, verrà utilizzata la versione già scaricata per evitare di scaricare nuovamente i dati. Gli oggetti di tipo dataset (ne vedremo altri oltre a MNIST) si comportano in maniera simile a una lista. E' possibile determinare il numero di elementi mediante la funzione `len`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di campioni di training: 60000\n",
      "Numero di campioni di test: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero di campioni di training:\",len(mnist_train))\n",
    "print(\"Numero di campioni di test:\",len(mnist_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo accedere agli elementi del dataset con l'indicizzazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7F662DA4A470>, 5)\n",
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7F662DA4A438>, 7)\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train[0])\n",
    "print(mnist_test[0])\n",
    "print(type(mnist_train[0][0]))\n",
    "print(type(mnist_train[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli elementi del dataset sono delle tuple contenenti due elementi:\n",
    " * Una immagine di tipo `PIL.Image`;\n",
    " * L'etichetta (es., $5$, $7$) dell'immagine.\n",
    " \n",
    "Si noti però che, a differenza di quanto avviene con una lista, `MNIST` non contiene in memoria tutti i campioni, ma li carica da file dinamicamente quando essi vengono richiesti. Ciò permette di lavorare efficientemente con dataset molto grandi (anche milioni di immagini). Possiamo visualizzare le immagini di PIL mediante matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#cmap='gray' serve per evitare di visualizzare le immagini in falsi colori\n",
    "plt.imshow(mnist_train[0][0],cmap='gray') \n",
    "plt.title(\"Classe: \"+str(mnist_train[0][1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per lavorare su immagini, dobbiamo prima trasformarle in tensori. Ciò si può fare convertendo prima l'immagine in un array di numpy, poi in un tensore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im=torch.from_numpy(np.array(mnist_train[0][0]))\n",
    "im.shape #l'immagine è un tensore 28x28 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa operazione dovrà essere effettuata su tutti gli elementi del dataset. Per automatizzare questa operazione, PyTorch permette di specificare una funzione di trasformazione, ovvero una funzione che verrà applicata a tutti gli elementi del dataset \"al volo\" quando questi vengono richiesti:\n",
    "\n",
    "Per trasformare le immagini di PIL in tensori di PyTorch, specifichiamo l'oggetto `torchvision.transforms.ToTensor()` come trasformazione al costruttore di MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "mnist_train = MNIST(root='data',train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = MNIST(root='data',train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo di che tipo sono gli elementi del dataset adesso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "sample =  mnist_train[0]\n",
    "print(type(sample[0]))\n",
    "print(type(sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo qual è la shape del tensore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tratta di un tensore $1 \\times 28 \\times 28$, dove $1$ indica che l'immagine è in scala di grigi (le immagini RGB hanno $3$ canali), mentre $28 \\times 28$ indica le dimensioni delle immagini. La dimensione aggiuntiva \"1\" viene aggiunta in automatico dall'oggetto `ToTensor` per compatibilità con le immagini a colori. Proviamo a mostrare alcune immagini utilizzando la libreria matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x396 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5.5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.title(\"Classe %d\" % mnist_train[i][1])\n",
    "    plt.imshow(mnist_train[i][0].squeeze().numpy(),cmap='gray') \n",
    "    #`squeeze` serve a trasformare il tensore 1 x 28 x 28 in un tensore 28 x 28\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per poter allenare un classificatore su questi dati, abbiamo bisogno di normalizzarli in modo che essi abbiano media nulla e deviazione standard unitaria. Calcoliamo media e varianza dei pixel contenuti in tutte le immagini del training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "for sample in mnist_train:\n",
    "    m+=sample[0].sum() #accumuliamo la somma di tutti i pixel\n",
    "\n",
    "#dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "m=m/(len(mnist_train)*28*28)\n",
    "    \n",
    "#procedura simile per calcolare la deviazione standard\n",
    "s=0\n",
    "for sample in mnist_train:\n",
    "    s+=((sample[0]-m)**2).sum()\n",
    "    \n",
    "s=np.sqrt(s/(len(mnist_train)*28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I valori trovati sono i seguenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.1307\n",
      "Std: 0.3081\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: %0.4f\"%m)\n",
    "print(\"Std: %0.4f\"%s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Va notato che questa procedura può essere effettuata per qualsiasi dataset ed è necessario effettuarla solo una volta. In seguito è possibile conservare questi valori e utilizzarli direttamente per normalizzare i dati. Possiamo dunque normalizzare un singolo campione come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimo: tensor(-0.4241)\n",
      "Massimo: tensor(2.8215)\n",
      "Media: tensor(-0.0134)\n",
      "Dev. Std.: tensor(0.9860)\n"
     ]
    }
   ],
   "source": [
    "sample=(mnist_train[15][0]-m)/s\n",
    "print(\"Minimo:\",sample.min())\n",
    "print(\"Massimo:\",sample.max())\n",
    "print(\"Media:\",sample.mean())\n",
    "print(\"Dev. Std.:\",sample.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per evitare di effettuare questa operazione in maniera manuale, possiamo specificare l'oggetto `transforms.Normalize` come trasformazione all'oggetto dataset. Abbiamo però bisogno di combinare la nuova trasformazione con la trasformazione `ToTensor`. Per farlo usiamo l'oggetto `transforms.Compose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compose prende in input una lista di trasformazioni\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((m,),(s,))])\n",
    "mnist_train = MNIST(root='data',train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root='data',train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo qualche statistica su uno degli elementi del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimo: tensor(-0.4241)\n",
      "Massimo: tensor(2.8215)\n",
      "Media: tensor(-0.0134)\n",
      "Dev. Std.: tensor(0.9860)\n"
     ]
    }
   ],
   "source": [
    "sample=mnist_train[15][0]\n",
    "print(\"Minimo:\",sample.min())\n",
    "print(\"Massimo:\",sample.max())\n",
    "print(\"Media:\",sample.mean())\n",
    "print(\"Dev. Std.:\",sample.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni immagine è adesso rappresentata da un tensore $1 \\times 28 \\times 28$. Tuttavia finora abbiamo visto solo algoritmi (es. regressore softmax) che lavorano su vettori unidimensionali di dati. Per poter lavorare sulle immagini con questi metodi, possiamo trasformare le immagini $28 \\times 28$ in vettori di $784$ dimensioni come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "sample = sample.view(-1)\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vogliamo effettuare anche questa operazione in automatico mediante le trasformazioni, possiamo definirne una \"custom\" utilizzando la funzione `torchvision.Lambda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((m,),(s,)),\n",
    "                               transforms.Lambda(lambda x: x.view(-1))])#specifichiamo l'operazione \"custom\"\n",
    "mnist_train = MNIST(root='data',train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root='data',train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train[0][0].shape)\n",
    "print(mnist_train[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Loader\n",
    "Ogni campione ottenuto mediante l'oggetto dataset MNIST verrà automaticamente normalizzato e trasformato in un vettore. Per effettuare l'ottimizzazione mediante Stochastic Gradient Descent, dobbiamo suddividere i campioni in mini-batch. Inoltre, è importante fornire i campioni in ordine casuale, in quanto fornire consecutivamente elementi con caratteristiche simili (es. stessa classe) favorirebbe l'overfitting. PyTorch ci permette di gestire il \"batching\" in automatico e in maniera multithread mediante l'oggetto `DataLoader`. Utilizziamo un batch size di $256$ immagini e due thread paralleli per velocizzare il caricamento dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=256, num_workers=2, shuffle=True)\n",
    "#shuffle permette di accedere ai dati in maniera casuale\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=256, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I data loader sono degli oggetti iterabili. Possiamo dunque accedere ai diversi batch in maniera sequenziale all'interno di un ciclo for. Il ciclo terminerà quando tutti i batch del dataset saranno stati caricati. Proviamo ad accedere al primo batch e interrompiamo il ciclo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 784])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for batch in mnist_test_loader:\n",
    "    break\n",
    "    \n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il batch contiene $256$ vettori di training di dimensione $784$ e altrettante etichette corrisponenti. Se il numero di elementi del dataset non è un multiplo del batch size, l'ultimo batch sarà di dimensioni inferiori. Proviamo ad iterare tutto il loader e vediamo quanti elementi contiene l'ultimo batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for batch in mnist_test_loader:\n",
    "    pass\n",
    "    \n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Salvataggio e Caricamento di Modelli\n",
    "\n",
    "Quando si allenano modelli su grandi dataset, la procedura di allenamento può essere molto lenta. Risulta dunque conveniente poter salvare su disco i modelli in modo da poterli caricare e riutilizzare in seguito. PyTorch permette di salvare e caricare modelli in maniera semplice. Il salvataggio viene effettuato serializzando tutti i parametri. E' possibile accedere a un dizionario contenente tutti i parametri del modello utilizzando il metodo `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear.weight', 'linear.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict=model.state_dict()\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel nostro caso si tratta di soli due elementi, ma in generale potrebbero essere di più. Possiamo dunque salvare il dizionario mediante `torch.save`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ripristinare lo stato del modello, dobbiamo prima costruire l'oggetto e poi utilizzare il metodo `load_state_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SoftMaxRegressor(4,3)\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Allenamento su GPU\n",
    "\n",
    "Dato che l'allenamento di un modello su grandi quantità di dati può essere lento, risulta conveniente velocizzare i calcoli effettuando l'allenamento su GPU, qualora una GPU dovesse essere disponibile nel sistema. Vediamo alcuni semplici passi per convertire il codice di training in questo senso.\n",
    "\n",
    "E' possibile verificare qualora una GPU sia disponibile nel sistema come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo dunque costruire una variabile `device` che sia uguale a `cpu` se non c'è nessuna GPU disponibile e `cuda` altrimenti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto, dobbiamo \"portare\" il modello che utilizzeremo sul device corretto. Possiamo farlo come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftMaxRegressor(\n",
       "  (linear): Linear(in_features=4, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La stessa operazione va effettuata su ciascun tensore con il quale lavoreremo, come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_norm.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queste modifiche ci permetteranno di utilizzare la GPU automaticamente se disponibile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Log di loss e accuracy e SGD\n",
    "\n",
    "Quando si effettua un allenamento mediante stochastic gradient descent, risulta un po' meno immedianto effettuare il log di loss e accuarcy. In genere, abbiamo conservato i valori di loss e accuracy calcolati su training set e test set alla fine di ogni epoca. Tuttavia, noi calcoleremo esplicitamente loss e accuracy sui singoli batch.\n",
    "\n",
    "Per ottenere delle stime valide per ogni epoca, accumuleremo i valori di loss e accuracy batch per batch e calcoleremo la media di questi valori pesata sulla base del numero di elementi contenuti in quel batch. Supponiamo di avere 3 batch nel dataset, la loss totale va calcolata come segue:\n",
    "\n",
    "\\begin{equation}\n",
    "L = \\frac{l_0\\cdot n_0 + l_1 \\cdot n_1 + l_2 \\cdot n_2}{n_0+n_1+n_2}\n",
    "\\end{equation}\n",
    "\n",
    "dove $l_i$ è la loss calcolata al batch i-esimo e $n_i$ è il numero di elementi contenuti nel batch i-esimo. E' inoltre possibile calcolare la loss \"parziale\" ad un batch diverso dal batch finale come segue:\n",
    "\n",
    "\\begin{equation}\n",
    "L_1 = \\frac{l_0\\cdot n_0 + l_1 \\cdot n_1}{n_0+n_1}\n",
    "\\end{equation}\n",
    "\n",
    "dove $L_1$ è la loss parziale calcolata fino al batch 1. Questi calcoli possono essere automatizzati mediante un oggetto chiamato `Meter`. Scriviamone uno molto semplice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageValueMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.num = 0\n",
    "    \n",
    "    def add(self, value, num):\n",
    "        self.sum += value*num\n",
    "        self.num += num\n",
    "        \n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.sum/self.num\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come utilizzare la classe appena definita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media pesata calcolata manualmente: 4.3\n",
      "Media pesata calcolata mediante meter: 4.3\n"
     ]
    }
   ],
   "source": [
    "meter = AverageValueMeter()\n",
    "\n",
    "meter.add(10,3) #inseriamo il valore 10, calcolato da un batch di 3 elementi\n",
    "meter.add(3,5) #inseriamo il valore 3, calcolato da un batch di 5 elementi\n",
    "meter.add(-1,2) #inseriamo il valore -1, calcolato da un batch di 2 elementi\n",
    "\n",
    "#media pesata calcolata manualmente:\n",
    "print('Media pesata calcolata manualmente:',(10*3+3*5-1*2)/(3+5+2))\n",
    "print('Media pesata calcolata mediante meter:',meter.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E' possibile ripristinare un meter ai valori iniziali mediante il metodo `reset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "meter.reset()\n",
    "print(meter.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il meter creato ci permette di ottenere una stima corrente di loss e accuracy. Dato che una epoca di training può richiedere anche diverse ore a seconda del dataset e del momento, loggeremo queste stime ad ogni iterazione. In questo caso, utilizzeremo come `global_step` il numero totale di campioni attualmente \"visti\" durante il training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Allenamento mediante SGD\n",
    "Alleniamo adesso mediante Stochastic Gradient Descent un regressore Softmax per classificare gli elementi del dataset `MNIST`. Utilizzeremo due cicli for, uno esterno per iterare lungo le epoche e uno interno per iterare lungo i batch. Il resto della procedura di training resta uguale. Per monitorare il training, all'interno di ogni epoca effettueremo un ciclo di training e un ciclo di test. Con dataset grandi, la procedura di training può essere lunga. Pertanto salveremo una copia del modello ad ogni iterazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo adesso una funzione di training che possiamo riutilizzare in seguito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "def train_classifier(model, train_loader, test_loader, exp_name='experiment', lr=0.001, epochs=10, momentum=0.9, logdir='logs'):\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = SGD(model.parameters(), lr, momentum=momentum) \n",
    "    #meters\n",
    "    loss_meter = AverageValueMeter()\n",
    "    acc_meter = AverageValueMeter()\n",
    "    #writer\n",
    "    writer = SummaryWriter(join(logdir, exp_name))\n",
    "    #device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    #definiamo un dizionario contenente i loader di training e test\n",
    "    loader = {\n",
    "        'train' : train_loader,\n",
    "        'test' : test_loader\n",
    "    }\n",
    "    #inizializziamo il global step\n",
    "    global_step = 0\n",
    "    for e in range(epochs):\n",
    "        #iteriamo tra due modalità: train e test\n",
    "        for mode in ['train','test']:\n",
    "            loss_meter.reset(); acc_meter.reset()\n",
    "            model.train() if mode == 'train' else model.eval()\n",
    "            with torch.set_grad_enabled(mode=='train'): #abilitiamo i gradienti solo in training\n",
    "                for i, batch in enumerate(loader[mode]):\n",
    "                    x=batch[0].to(device) #\"portiamoli sul device corretto\"\n",
    "                    y=batch[1].to(device)\n",
    "                    output = model(x)\n",
    "                    \n",
    "                    #aggiorniamo il global_step\n",
    "                    #conterrà il numero di campioni visti durante il training\n",
    "                    n = x.shape[0] #numero di elementi nel batch\n",
    "                    global_step += n\n",
    "                    l = criterion(output,y)\n",
    "\n",
    "                    if mode=='train':\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    acc = accuracy_score(y.to('cpu'),output.to('cpu').max(1)[1])\n",
    "                    loss_meter.add(l.item(),n)\n",
    "                    acc_meter.add(acc,n)\n",
    "\n",
    "                    #loggiamo i risultati iterazione per iterazione solo durante il training\n",
    "                    if mode=='train':\n",
    "                        writer.add_scalar('loss/train', loss_meter.value(), global_step=global_step)\n",
    "                        writer.add_scalar('accuracy/train', acc_meter.value(), global_step=global_step)\n",
    "            #una volta finita l'epoca (sia nel caso di training che test, loggiamo le stime finali)\n",
    "            writer.add_scalar('loss/' + mode, loss_meter.value(), global_step=global_step)\n",
    "            writer.add_scalar('accuracy/' + mode, acc_meter.value(), global_step=global_step)\n",
    "            \n",
    "        #conserviamo i pesi del modello alla fine di un ciclo di training e test\n",
    "        torch.save(model.state_dict(),'%s-%d.pth'%(exp_name,e+1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo adesso il modello come segue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 10\n",
    "momentum = 0.9\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((m,),(s,)),\n",
    "                               transforms.Lambda(lambda x: x.view(-1))])\n",
    "\n",
    "mnist_train = MNIST(root='data',train=True, download=True, transform=transform)\n",
    "mnist_test = MNIST(root='data',train=False, download=True, transform=transform)\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=256, num_workers=2, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=256, num_workers=2)\n",
    "\n",
    "model = SoftMaxRegressor(784, 10)\n",
    "\n",
    "model = train_classifier(model, mnist_train_loader, mnist_test_loader, 'SGD-softmax-regressor', lr=lr, epochs=epochs, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alla fine del training, su tensorboard dovremmo osservare un grafico del genere:\n",
    "\n",
    "<center>\n",
    "    <img width=70% src='img/mnist_sgd_tb.jpg'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 6**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Guardando i grafici ottenuti, sembra possibile migliorare l'accuracy di test allenando il modello per un numero maggiore di epoche? Perché?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 6**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per valutare le performance del modello, dobbiamo prima ottenere le predizioni per ciascuno degli elementi di test. Per farlo, definiamo la seguente funzione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    predictions, labels = [], []\n",
    "    for batch in loader:\n",
    "        x = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "        output = model(x)\n",
    "        preds = output.to('cpu').max(1)[1].numpy()\n",
    "        labs = y.to('cpu').numpy()\n",
    "        predictions.extend(list(preds))\n",
    "        labels.extend(list(labs))\n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo dunque ottenere le predizioni di training e test e valutare il modello con le misure di valutazioni che preferiamo. Ad esempio, mediante accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy di training: 0.9285\n",
      "Accuarcy di test: 0.9228\n"
     ]
    }
   ],
   "source": [
    "predictions_train, labels_train = test_classifier(model, mnist_train_loader)\n",
    "predictions_test, labels_test = test_classifier(model, mnist_test_loader)\n",
    "print(\"Accuarcy di training: %0.4f\"% accuracy_score(labels_train, predictions_train))\n",
    "print(\"Accuarcy di test: %0.4f\"% accuracy_score(labels_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Multilayer Perceptron (MLP)\n",
    "\n",
    "Finora abbiamo visto dei semplici modelli lineari per classificazione e regressione. Tali modelli possono essere estesi in maniera molto semplice a modelli più \"profondi\", quali il MultiLayer Perceptron (MLP) mediante PyTorch.\n",
    "\n",
    "Una rete di tipo MLP è composta da tre livelli:\n",
    " * Un livello di \"input\" nel quale verranno presentati i dati in ingresso;\n",
    " * Un livello \"nascosto\" che conterrà delle rappresentazioni \"latenti\" dei dati;\n",
    " * Un livello di \"uscita\" che conterrà i dati di output (le etichette regresse).\n",
    " \n",
    "<center>\n",
    "<img src=\"img/mlp.png\" width=600px>\n",
    "</center>\n",
    "\n",
    "Il livello \"nascosto\" è detto tale in quanto durante il training non viene esercitato un controllo diretto sui valori che esso assume (la loss è applicata al layer di output). Si dice pertanto che il livello apprende una \"rappresentazione latente\".\n",
    "\n",
    "Dei tre livelli, gli unici due che contengono parametri che possono essere appresi sono il secondo e il terzo. Pertanto si dice spesso che un MLP di questo tipo contiene solo \"due layer\".\n",
    "\n",
    "Nel caso in cui volessimo implementare un regressore, l'ultimo livello del MLP conterrebbe i valori da regredire. Nel caso in cui volessimo implementare un regressore softmax (ovvero un classificatore), l'ultimo livello deve contenere un numero di nodi pari al numero di classi e i valori assunti da questi nodi sono da considerare come dei logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fashion-MNIST\n",
    "Vedremo degli esempi di classificatori MLP su un'altro dataset simile a MNIST in dimensioni ma più complesso in termini di contenuto visuale: Fashion-MNIST.\n",
    "\n",
    "Fashion-MNIST è un dataset introdotto da Zalando nel 2017. Il dataset è progettato per essere compatto in maniera simile a MNIST-DIGITS, in modo da poter essere utilizzato in maniera agevole per effettuare esperimenti veloci (utili quando si vuole appurare la bontà di un'idea). Allo stesso tempo, il problema di classificazione proposto con il dataset è molto più complesso di quello relativo a MNIST-DIGITS. In maniera del tutto simile a MNIST-DIGITS, il dataset contiene $60,000$ immagini di training e $10,000$ immagini di testing grandi $28 \\times 28$ pixels. Le immagini sono suddivise in $10$ classi relative al mondo della moda, come riassunto di seguito.\n",
    "\n",
    "<center>\n",
    "<img width=800px src=\"img/fashion.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similmente a quanto avviene per MNIST-DIGITS, PyTorch mette a disposizioni un oggetto per permettere di caricare agevolmente i dati. Carichiamo il dataset e visualizziamo qualche esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb12b5e4c8ea49ff8b22ef552602cec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion/FashionMNIST/raw/train-images-idx3-ubyte.gz to fashion/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to fashion/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c528a393f3cc4b0897a024d09e9f5da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion/FashionMNIST/raw/train-labels-idx1-ubyte.gz to fashion/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to fashion/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8d69185b4d44b5aff75d378683a3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to fashion/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to fashion/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37b535a7f0a46c794321999029f34cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fashion/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to fashion/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "fashion_train = FashionMNIST(root='fashion',train=True, download=True, transform=transforms.ToTensor())\n",
    "fashion_test = FashionMNIST(root='fashion',train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for i in range(14):\n",
    "    plt.subplot(2,7,i+1)\n",
    "    plt.title(\"Classe %d\" % fashion_train[i][1])\n",
    "    plt.imshow(fashion_train[i][0].squeeze().numpy(),cmap='gray') \n",
    "    #`squeeze` serve a trasformare il tensore 1 x 28 x 28 in un tensore 28 x 28\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 7**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Cosa rende MNIST-Fashion più \"complesso\" di MNIST-DIGITS? Fare un esempio considerando qualcuno dei campioni visualizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 7**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo i dataloaders con le opportune trasformazioni. Utilizzeremo la media e deviazione standard relative al dataset, ovvero $0.2860$ e $0.3530$ rispettivamente (sono state pre-computate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8d23110a2849b8bcc39f1244867346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69af57e8294c4a92a02c7b80b9c3968a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a5251b13e14e7daa0c95ca6df1dd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db4bc103fa143739c980e46c58fd177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), #conversione in tensore\n",
    "                                #normalizzazione per media e deviazione standard \n",
    "                                #(0.2860 e 0.3530 sono media e dev standard del dataset)\n",
    "                                transforms.Normalize((0.2860,),(0.3530,)),\n",
    "                                #trasformiamo l'immagine 28x28 in un vettore di 784 componenti\n",
    "                               transforms.Lambda(lambda x: x.view(-1))])\n",
    "\n",
    "#ridefiniamo i dataset specificando le trasformazioni\n",
    "fashion_train = FashionMNIST(root='data',train=True, download=True, transform=transform)\n",
    "fashion_test = FashionMNIST(root='data',train=False, download=True, transform=transform)\n",
    "\n",
    "#definiamo i dataloaders\n",
    "fashion_train_loader = DataLoader(fashion_train, batch_size=256, num_workers=2, shuffle=True)\n",
    "#shuffle permette di accedere ai dati in maniera casuale\n",
    "fashion_test_loader = DataLoader(fashion_test, batch_size=256, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniziamo ad esplorare il dataset allenando un regressore softmax con il codice di training scritto in precedenza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fashion_softmax_regressor = SoftMaxRegressor(784, 10)\n",
    "fashion_softmax_regressor = train_classifier(fashion_softmax_regressor,fashion_train_loader, \\\n",
    "                                             fashion_test_loader, 'fashion_softmax_regressor',\\\n",
    "                                             lr=0.01,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otteniamo accuracy di training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion Softmax Regressor - Accuarcy di training: 0.8613\n",
      "Fashion Softmax Regressor - Accuarcy di test: 0.8377\n"
     ]
    }
   ],
   "source": [
    "predictions_train_fashion_softmax_regressor, labels_train_fashion = test_classifier(fashion_softmax_regressor, fashion_train_loader)\n",
    "predictions_test_fashion_softmax_regressor, labels_test_fashion = test_classifier(fashion_softmax_regressor, fashion_test_loader)\n",
    "print(\"Fashion Softmax Regressor - Accuarcy di training: %0.4f\"% accuracy_score(labels_train_fashion, predictions_train_fashion_softmax_regressor))\n",
    "print(\"Fashion Softmax Regressor - Accuarcy di test: %0.4f\"% accuracy_score(labels_test_fashion, predictions_test_fashion_softmax_regressor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 8**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si osservino i grafici ottenuti su tensorboard. Il modello è arrivato a convergenza? E' possibile ottenere risultati migliori allenandolo per più epoche?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 8**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Classificatore MLP\n",
    "Vediamo come implementare un semplice classificatore MLP. Utilizzeremo la tangente iperbolica come funzione di attivazione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, out_classes):\n",
    "        \"\"\"Costruisce un classificatore MLP.\n",
    "            Input:\n",
    "                in_features: numero di feature in input (es. 784)\n",
    "                hidden_units: numero di unità nel livello nascosto (es. 512)\n",
    "                out_classes: numero di classi in uscita (es. 10)\"\"\"\n",
    "        super(MLPClassifier, self).__init__() \n",
    "        self.hidden_layer = nn.Linear(in_features, hidden_units)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.output_layer = nn.Linear(hidden_units, out_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        hidden_representation = self.hidden_layer(x)\n",
    "        hidden_representation = self.activation(hidden_representation)\n",
    "        scores = self.output_layer(hidden_representation)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 9**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confronti il codice scritto sopra con quello del SoftMax regressor. Quali sono le principali differenze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 9**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleniamo adesso il classificatore MLP su Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mlp_classifier = MLPClassifier(784, 512, 10)\n",
    "fashion_mlp_classifier = train_classifier(fashion_mlp_classifier,fashion_train_loader, \\\n",
    "                                             fashion_test_loader, 'fashion_mlp_classifier',\\\n",
    "                                             lr=0.01,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo accuracy di training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion MLP Classifier - Accuarcy di training: 0.8979\n",
      "Fashion MLP Classifier - Accuarcy di test: 0.8748\n"
     ]
    }
   ],
   "source": [
    "predictions_train_fashion_mlp_classifier, labels_train = test_classifier(fashion_mlp_classifier, fashion_train_loader)\n",
    "predictions_test_fashion_mlp_classifier, labels_test = test_classifier(fashion_mlp_classifier, fashion_test_loader)\n",
    "print(\"Fashion MLP Classifier - Accuarcy di training: %0.4f\"% accuracy_score(labels_train, predictions_train_fashion_mlp_classifier))\n",
    "print(\"Fashion MLP Classifier - Accuarcy di test: %0.4f\"% accuracy_score(labels_test, predictions_test_fashion_mlp_classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 10**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confrontino le performance ottenute dal MLP con quelle ottenute dal Softmax regeressor. Quale dei due modelli raggiunge risultati migliori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 10**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Classificatore MLP \"Profondo\" (Deep MLP)\n",
    "E' possibile costruire un classificatore MLP con un numero arbitrario di livelli. In generere, quando il MLP ha più di 3 livelli, esso viene detto \"prondo\" (deep). Ad esempio un MLP con due livelli nascosti (4 livelli in tutto) può essere rappresentato come segue:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/deep_mlp.png\" width=600px>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si noti che ad ogni livello nascosto segue la funzione di attivazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 11**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Perché è necessario inserire le funzioni di attivazione dopo ciascun livello nascosto? Potremmo farne a meno?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 11**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementiamo il classificatore MLP profondo. Dato che in un MLP l'output di un livello è sempre l'input del livello successivo, possiamo implementare in maniera compatta il modello utilizzando il modulo `nn.Sequential` che permette di connettere moduli in cascata. Ad esempio, il seguente codice:\n",
    "\n",
    "```python\n",
    "y = modulo1(x)\n",
    "z = modulo2(y)\n",
    "h = modulo3(h)\n",
    "```\n",
    "\n",
    "è implementato come segue con `nn.Sequential`:\n",
    "\n",
    "```python\n",
    "seq = nn.Sequential(modulo1, modulo2, modulo3)\n",
    "h = seq(x)\n",
    "```\n",
    "\n",
    "Definiamo la classe `DeepMLPClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLPClassifier(nn.Module):\n",
    "    def __init__(self, in_features, hidden_units, out_classes):\n",
    "        \"\"\"Costruisce un classificatore MLP \"profondo\".\n",
    "            Input:\n",
    "                in_features: numero di feature in input (es. 784)\n",
    "                hidden_units: numero di unità nei livelli nascosti (es. 512)\n",
    "                out_classes: numero di classi in uscita (es. 10)\"\"\"\n",
    "        super(DeepMLPClassifier, self).__init__() \n",
    "        \n",
    "        self.model = nn.Sequential(nn.Linear(in_features, hidden_units),\n",
    "                                               nn.Tanh(),\n",
    "                                               nn.Linear(hidden_units, hidden_units),\n",
    "                                               nn.Tanh(),\n",
    "                                               nn.Linear(hidden_units, out_classes))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso alleniamo il modello su Fashion-MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_deep_mlp_classifier = DeepMLPClassifier(784, 512, 10)\n",
    "fashion_deep_mlp_classifier = train_classifier(fashion_deep_mlp_classifier,fashion_train_loader, \\\n",
    "                                             fashion_test_loader, 'fashion_deep_mlp_classifier',\\\n",
    "                                             lr=0.01,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcoliamo adesso accuracy di training e test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion Deep MLP Classifier - Accuarcy di training: 0.8992\n",
      "Fashion Deep MLP Classifier - Accuarcy di test: 0.8716\n"
     ]
    }
   ],
   "source": [
    "predictions_train_fashion_deep_mlp_classifier, labels_train = test_classifier(fashion_deep_mlp_classifier, fashion_train_loader)\n",
    "predictions_test_fashion_deep_mlp_classifier, labels_test = test_classifier(fashion_deep_mlp_classifier, fashion_test_loader)\n",
    "print(\"Fashion Deep MLP Classifier - Accuarcy di training: %0.4f\"% accuracy_score(labels_train, predictions_train_fashion_deep_mlp_classifier))\n",
    "print(\"Fashion Deep MLP Classifier - Accuarcy di test: %0.4f\"% accuracy_score(labels_test, predictions_test_fashion_deep_mlp_classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Domanda 12**\n",
    "<img src=\"img/qmark.jpg\" style=\"width:150px; float:left;\"/>\n",
    "\n",
    "Si confrontino i risultati con quelli ottenuti mediante MLP. Quale dei due modelli raggiunge risultati migliori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Risposta 12**\n",
    "<img style=\"float: left;width:150px;\" src=\"img/note.png\">\n",
    "\n",
    "<div style=\"background-color:#efefef; margin-left:150px; border:solid 1px; border-color:#dddddd; border-radius: 3px;\">\n",
    "<br><br><br><br><br><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Costruzione di un Oggetto Dataset Personalizzato\n",
    "\n",
    "Abbiamo visto che PyTorch mette a disposizione alcuni oggetti dataset (ad esempio MNIST) che permettono di caricare determinati set di dati. Se vogliamo allenare un algoritmo su un set di dati non incluso in PyTorch, dobbiamo costruire un oggetto Dataset che ci permetta di caricare i dati. Inizieremo considerando il dataset disponibile al seguente URL:\n",
    "\n",
    "<center>http://people.csail.mit.edu/torralba/code/spatialenvelope/</center>\n",
    "\n",
    "\n",
    "Si tratta di un dataset contenente $2688$ immagini a colori di dimensioni $256 \\times 256$, suddivise in $8$ classi a seconda del tipo di scena ritratto:\n",
    " 1. coast\n",
    " * forest\n",
    " * highway\n",
    " * insidecity\n",
    " * mountain\n",
    " * opencountry\n",
    " * street\n",
    " * tallbuilding\n",
    " \n",
    "Il dataset si può scaricare da:\n",
    "<center><a href=\"http://people.csail.mit.edu/torralba/code/spatialenvelope/spatial_envelope_256x256_static_8outdoorcategories.zip\">http://people.csail.mit.edu/torralba/code/spatialenvelope/spatial_envelope_256x256_static_8outdoorcategories.zip</a>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Scarichiamo il dataset e estraiamo i file nella cartella `8scenes` nella directory di lavoro. La cartella conterrà le $2688$ immagini a colore. La classe di appartenenza di ogni immagine è inclusa nel nome del file (ad es. `coast_bea9.jpg`). Il dataset non è suddiviso in training e test set. Per lavorare sullo stesso training/testing split, scarichiamo l'archivio zip disponibile all'URL:\n",
    "\n",
    "<center><a href=\"http://iplab.dmi.unict.it/furnari/downloads/8scenes_train_test_split.zip\">http://iplab.dmi.unict.it/furnari/downloads/8scenes_train_test_split.zip</a>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Estraiamo il contenuto dell'archivio all'interno della cartella `8scenes`. L'archivio contiene tre file:\n",
    "\n",
    " * `train.txt`: contiene i nomi di $2188$ immagini di training con le relative etichette in formato numerico ($0-7$);\n",
    " * `test.txt`: contiene i nomi delle rimanenti $500$ immagini di testing con le relative etichette in formato numerico;\n",
    " * `classes.txt`: contiene i nomi delle classi. In particolare, la riga i-esima conterrà il nome della classe i-esima (dunque il nome della classe indicata come $0$ in `train.txt` e `test.txt` sarà contenuto nella prima riga del file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Costruiremo adesso un oggetto Dataset che ci permetta di caricare le immagini di training e test. Ciò si può fare in maniera molto naturale in PyTorch ereditando dalla classe `Dataset`. Ogni oggetto `Dataset` deve contenere almeno i seguenti metodi:\n",
    " * Un costruttore;\n",
    " * Il metodo `__len__`, che restituisce il numero di elementi contenuti nel dataset;\n",
    " * Il metodo `__getitem__`, che prende in input un indice $i$ e restituisce l'$i$-esimo elemento del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "\n",
    "class ScenesDataset(Dataset):\n",
    "    \"\"\"Implementa l'oggetto ScenesDataset che ci permette di caricare\n",
    "    le immagini del dataset 8 Scenes\"\"\"\n",
    "    def __init__(self,base_path,txt_list,transform=None):\n",
    "        \"\"\"Input:\n",
    "            base_path: il path alla cartella contenente le immagini\n",
    "            txt_list: il path al file di testo contenente la lista delle immagini\n",
    "                        con le relative etichette. Ad esempio train.txt o test.txt.\n",
    "            transform: implementeremo il dataset in modo che esso supporti le trasformazioni\"\"\"\n",
    "        #conserviamo il path alla cartella contenente le immagini\n",
    "        self.base_path=base_path \n",
    "        #carichiamo la lista dei file\n",
    "        #sarà una matrice con n righe (numero di immagini) e 2 colonne (path, etichetta)\n",
    "        self.images = np.loadtxt(txt_list,dtype=str,delimiter=',')\n",
    "        #conserviamo il riferimento alla trasformazione da applicare\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #recuperiamo il path dell'immagine di indice index e la relativa etichetta\n",
    "        f,c = self.images[index]\n",
    "        \n",
    "        #carichiamo l'immagine utilizzando PIL\n",
    "        im = Image.open(path.join(self.base_path, f))\n",
    "        \n",
    "        #se la trasfromazione è definita, applichiamola all'immagine\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "        #convertiamo l'etichetta in un intero\n",
    "        label = int(c)\n",
    "\n",
    "        #restituiamo un dizionario contenente immagine etichetta\n",
    "        return {'image' : im, 'label':label}\n",
    "\n",
    "    #restituisce il numero di campioni: la lunghezza della lista \"images\"\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo adesso a istanziare il dataset per caricare dei dati:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transforms.ToTensor())\n",
    "sample = dataset[0]\n",
    "#l'immagine è 3 x 256 x 256 perché è una immagine a colori\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le immagini sono di dimensione $256 \\times 256$. Per ridurre i tempi computazionali, potremmo voler lavorare con immagini più piccole. Possiamo utilizzare la trasformazione `Resize` per ridimensionarle a una dimensione predefinita, ad esempio $32 \\times 32$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32),transforms.ToTensor()])\n",
    "dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transform)\n",
    "sample = dataset[0]\n",
    "print(sample['image'].shape)\n",
    "print(sample['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per poter normalizzare i dati, calcoliamo medie e varianze di tutti i pixel contenuti nelle immagini del dataset. Nel caso di immagini a colori, tali valori sono spesso calcolati canale per canale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transforms.ToTensor())\n",
    "m = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    m+=sample['image'].sum(1).sum(1).numpy() #accumuliamo la somma dei pixel canale per canale\n",
    "\n",
    "#dividiamo per il numero di immagini moltiplicato per il numero di pixel\n",
    "m=m/(len(dataset)*256*256)\n",
    "    \n",
    "#procedura simile per calcolare la deviazione standard\n",
    "s = np.zeros(3)\n",
    "for sample in dataset:\n",
    "    s+=((sample['image']-torch.Tensor(m).view(3,1,1))**2).sum(1).sum(1).numpy()\n",
    "    \n",
    "s=np.sqrt(s/(len(dataset)*256*256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medie e deviazioni standard per i tre canali sono:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medie [0.42478882 0.45170449 0.4486707 ]\n",
      "Dev.Std. [0.25579564 0.2465238  0.27658251]\n"
     ]
    }
   ],
   "source": [
    "print(\"Medie\",m)\n",
    "print(\"Dev.Std.\",s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo inserire la corretta normalizzazione tra le trasformazioni. Ad esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(m,s),\n",
    "                               transforms.Lambda(lambda x: x.view(-1))])\n",
    "dataset = ScenesDataset('8scenes','8scenes/train.txt',transform=transform)\n",
    "\n",
    "print(dataset[0]['image'].shape)\n",
    "print(dataset[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 1**\n",
    "\n",
    "Si modifichi il codice che implementa il regressore softmax sul dataset degli Iris di Fisher per utilizzare la loss:\n",
    "$\\mathcal{L}_\\theta(\\mathbf{x},j) = - \\log f(\\mathbf{x})_j$. Si provi a ripetere l'addestramento per diversi learning rate e si confrontino i risultati ottenuti con la loss vista nel laboratorio. Ci sono delle differenze?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 2**\n",
    "\n",
    "Si confrontino le performance di un classificatore softmax con quelle di un classificatore one-vs-all che si basa su un regressore logistico nel caso delle iris di Fisher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 3**\n",
    "\n",
    "I modelli visti in questo laboratorio sono stati allenati per poche epoche. Ripetere l'analisi effettuata in questo laboratorio allenando i modelli per un numero di epoche sufficiente a farli convergere (si analizzino i grafici per dedurre quando i modelli sono arrivati a convergenza). Si confrontino i numeri di epoche necessarie per far convergere i diversi modelli. Quali ne richiedono di più? Quali ne richiedono di meno? Si valutino e confrontino le performance di tutti i modelli con gli score $F_1$ per classe e con la media di questi valor ($mF_1$), con le matrici di confusione e mediante l'accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 4**\n",
    "\n",
    "Si allenino un regressore lineare, un regressore MLP e un regressore MLP con due livelli nascosti sul dataset di regressore Boston. Si confrontino le performance dei modelli. Quale ottiene risultati più accurati?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 5**\n",
    "\n",
    "Si ripeta l'analisi fatta su Fashion-MNIST con Classificatore MLP e Deep-MLP sul dataset MNIST. Ri raggiungono conclusioni simili?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/code.png\" style=\"width:150px; margin-right:30px; float:left\">\n",
    "\n",
    "**Esercizio 6**\n",
    "\n",
    "Si costruisca un oggetto Dataset per caricare i dati utili al proprio progetto di Machine Learning. Si calcolino medie e deviazioni standard utili a normalizzare i dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    " * Documentazione di PyTorch. http://pytorch.org/docs/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
